{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "451ea3d6-5277-4301-8dcb-5082a5765ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1000.,  1760.,  2520.,  3280.,  4040.,  4800.,  5560.,  6320.,\n",
       "        7080.,  7840.,  8600.,  9360., 10120., 10880., 11640., 12400.,\n",
       "       13160., 13920., 14680., 15440., 16200., 16960., 17720., 18480.,\n",
       "       19240., 20000.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linspace(1000, 20000, 26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0fb8250c-c107-4be6-bc3c-a7a6b455386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDPIndepTester:\n",
    "    def __init__(self, cuda_device):\n",
    "        self.cuda_device = cuda_device\n",
    "   \n",
    "    def bin_separately(self, data_X, data_Y, kappa):\n",
    "        return (\n",
    "            self.bin(data_X, kappa),\n",
    "            self.bin(data_Y, kappa)\n",
    "            )\n",
    "       \n",
    "    def range_check(self, data):\n",
    "        if (torch.sum(data.gt(1))).gt(0):\n",
    "            print(\"check data range\")\n",
    "            return False\n",
    "        elif (torch.sum(data.lt(0))).gt(0):\n",
    "            print(\"check data range\")\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def run_test_conti_data(self, B, data_Y, data_Z, kappa, alpha, gamma, discrete_noise = False):\n",
    "        #0. data range check\n",
    "        \n",
    "        if not self.range_check(data_Y):\n",
    "            return\n",
    "        if not self.range_check(data_Z):\n",
    "            return\n",
    "        \n",
    "        #1. bin\n",
    "        n = data_Y.size(dim = 0)\n",
    "        data_Y_binned, data_Z_binned = self.bin_separately(data_Y, data_Z, kappa)\n",
    "        print(data_Y_binned)\n",
    "        #2. privatize\n",
    "        data_Y_priv, data_Z_priv, noise_var_Y, noise_var_Z = self.privatize_indep(\n",
    "            data_Y = data_Y_binned,\n",
    "            data_Z = data_Z_binned,\n",
    "            alpha = alpha,\n",
    "            discrete_noise = discrete_noise\n",
    "        )\n",
    "        print(data_Y_priv)\n",
    "        print(data_Z_priv)\n",
    "\n",
    "        #4 compute original u-stat\n",
    "        ustatOriginal = self.u_stat_indep_matrix_efficient(data_Y_priv, data_Z_priv)\n",
    "\n",
    "        print(f\"original u-statistic:{ustatOriginal}\")\n",
    "        \n",
    "        #permutation procedure\n",
    "        permStats = torch.empty(B).to(self.cuda_device)\n",
    "        \n",
    "        for i in range(B):\n",
    "            perm_stat_now = self.u_stat_indep_matrix_efficient(\n",
    "                data_Y_priv,\n",
    "                data_Z_priv[\n",
    "                    torch.randperm(data_Z_priv.size(dim=0))],\n",
    "                ).to(self.cuda_device)\n",
    "\n",
    "            permStats[i] = perm_stat_now\n",
    "            print(f\"perm_stat_now = {perm_stat_now}\")\n",
    "         \n",
    "        \n",
    "        p_value_proxy = (1 +\n",
    "                         torch.sum(\n",
    "                             torch.gt(input = permStats, other = ustatOriginal)\n",
    "                         )\n",
    "                        ) / (B + 1)\n",
    "        \n",
    "        \n",
    "        #print(f\"p value proxy: {p_value_proxy}\")\n",
    "        \n",
    "        return(p_value_proxy < gamma, noise_var_Y, noise_var_Z)#test result: TRUE = 1 = reject the null, FALSE = 0 = retain the null.\n",
    "\n",
    "        \n",
    "    def privatize_indep(self, data_Y, data_Z, alpha = float(\"inf\"), discrete_noise = False):\n",
    "        ## assume the data is discrete by nature or has already been dicretized.\n",
    "        n = data_Y.size(dim = 0) # Y and Z have the same sample size.\n",
    "        kappa_d1 = data_Y.size(dim = 1) #kappa^d if conti data, d if discrete data\n",
    "        kappa_d2 = data_Z.size(dim = 1) #kappa^d if conti data, d if discrete data\n",
    "\n",
    "        print(f\"noise dimension : {kappa_d1}, {kappa_d2}\")\n",
    "        \n",
    "        scale_factor = torch.tensor( (kappa_d1 * kappa_d2)**(1/2) )\n",
    "        sigma_kappa_alpha = 4 * (2 ** (1/2)) * scale_factor / alpha\n",
    "        \n",
    "        if alpha == float(\"inf\"): #non-private case\n",
    "            return( \n",
    "                    torch.mul(scale_factor, data_Y),\n",
    "                    torch.mul(scale_factor, data_Z),\n",
    "                    0, 0\n",
    "                     )\n",
    "        else:\n",
    "            data_Y_priv, noise_var_Y = self.privatize_indep_separate(\n",
    "                    data = data_Y,\n",
    "                    scale_factor = scale_factor,\n",
    "                    sigma_kappa_alpha = sigma_kappa_alpha,\n",
    "                    discrete_noise = discrete_noise\n",
    "            )\n",
    "            data_Z_priv, noise_var_Z = self.privatize_indep_separate(\n",
    "                    data = data_Z,\n",
    "                    scale_factor = scale_factor,\n",
    "                    sigma_kappa_alpha = sigma_kappa_alpha,\n",
    "                    discrete_noise = discrete_noise\n",
    "            )\n",
    "        return(data_Y_priv, data_Z_priv, noise_var_Y, noise_var_Z)\n",
    "        \n",
    "    \n",
    "    def privatize_indep_separate(self, data, scale_factor, sigma_kappa_alpha, discrete_noise):\n",
    "        n = data.size(dim = 0)\n",
    "        if discrete_noise:\n",
    "            noise, noise_var = self.noise_conti(data, sigma_kappa_alpha) #fix here later\n",
    "        else:\n",
    "            noise, noise_var = self.noise_conti(data, sigma_kappa_alpha)\n",
    "        return(   \n",
    "            torch.add(\n",
    "                input = noise.reshape(n, -1),\n",
    "                alpha = scale_factor,\n",
    "                other = data\n",
    "            ), \n",
    "            noise_var\n",
    "        )\n",
    "                       \n",
    "    def noise_conti(self, data, sigma_kappa_alpha):\n",
    "        #dim = kappa^d for conti data, d for discrete data\n",
    "        laplace_samples = self.generate_unit_laplace(data)\n",
    "        laplace_samples = sigma_kappa_alpha * laplace_samples\n",
    "        print(\"noise type: conti\")\n",
    "        return( laplace_samples, torch.var(laplace_samples) )\n",
    "    \n",
    "\n",
    "    def generate_unit_laplace(self, data):\n",
    "        n = data.size(dim = 0)\n",
    "        d = data.size(dim = 1)\n",
    "        unit_laplace_generator = torch.distributions.laplace.Laplace(\n",
    "            torch.tensor(0.0).to(self.cuda_device),\n",
    "            torch.tensor(2**(-1/2)).to(self.cuda_device)\n",
    "        )\n",
    "        return unit_laplace_generator.sample(sample_shape = torch.Size([n * d]))\n",
    "\n",
    "\n",
    "    def bin(self, data, kappa): \n",
    "        ''' Only for continuous data'''\n",
    "        \n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "     \n",
    "        # 1. for each dimension, turn the continuous data into interval\n",
    "        # each row now indicates a hypercube in [0,1]^d\n",
    "        # the more the data is closer to 1, the larger the interval index.\n",
    "        dataInterval = self.transform_bin_index(data = data, nIntervals = kappa)\n",
    "        \n",
    "        # 2. for each datapoint(row),\n",
    "        #    turn the hypercube data into a multivariate data of (1, 2, ..., kappa^d)\n",
    "        #    each row now becomes an integer.\n",
    "        dataMultivariate = self.TransformMultivariate(\n",
    "            dataInterval = dataInterval,\n",
    "            nBin = kappa,\n",
    "        )\n",
    "        # 3. turn the indices into one-hot vectors\n",
    "        dataOnehot = self.TransformOnehot(dataMultivariate, kappa**d)\n",
    "        return(dataOnehot)\n",
    "    \n",
    "    def transform_bin_index(self, data, nIntervals):\n",
    "        ''' Only for continuous data.\n",
    "        for each dimension, transform the data in [0,1] into the interval index\n",
    "        first interval = [0, x], the others = (y z]\n",
    "        \n",
    "        input arguments\n",
    "            data: torch tensor object on GPU\n",
    "            nIntervals: integer\n",
    "        output\n",
    "            dataIndices: torch tensor, dimension same as the input\n",
    "        '''\n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "        breaks = torch.linspace(start = 0, end = 1, steps = nIntervals + 1).to(self.cuda_device) #floatTensor\n",
    "        dataIndices = torch.bucketize(data, breaks, right = False) # ( ] form.\n",
    "        dataIndices = dataIndices.add(\n",
    "            dataIndices.eq(0)\n",
    "        ) #move 0 values from the bin number 0 to the bin number 1        \n",
    "        return(dataIndices)\n",
    "    \n",
    "    def TransformMultivariate(self, dataInterval, nBin):\n",
    "        \"\"\"Only for continuous and multivariate data .\"\"\"\n",
    "        d = self.get_dimension(dataInterval)\n",
    "        \n",
    "        if d == 1:\n",
    "            return(dataInterval.sub(1))\n",
    "        else:\n",
    "            exponent = torch.linspace(start = (d-1), end = 0, steps = d, dtype = torch.long)\n",
    "            vector = torch.tensor(nBin).pow(exponent)\n",
    "            return( torch.matmul( dataInterval.sub(1).to(torch.float), vector.to(torch.float).to(self.cuda_device) ).to(torch.long) )\n",
    "    \n",
    "    def TransformOnehot(self, dataMultivariate, newdim):\n",
    "        return(\n",
    "            torch.nn.functional.one_hot(\n",
    "                dataMultivariate,\n",
    "                num_classes = newdim)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def get_dimension(self, data):\n",
    "        if data.dim() == 1:\n",
    "            return(1)\n",
    "        elif data.dim() == 2:\n",
    "            return( data.size(dim = 1) )\n",
    "        else:\n",
    "            return # we only use up to 2-dimensional tensor, i.e. matrix\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def kernel_indep(self, fourchunk):\n",
    "        ip = torch.matmul(fourchunk, torch.transpose(fourchunk, 0, 1))\n",
    "        return(ip[0,1] + ip[2,3] - ip[0,2] - ip[1,3])\n",
    "\n",
    "     \n",
    "    def u_stat_indep_matrix(self, data_X, data_Y):\n",
    "        n = data_X.size(dim = 0)\n",
    "        n_four = n * (n-1) * (n-2) * (n-3)\n",
    "\n",
    "\n",
    "\n",
    "        Phi = torch.matmul(data_X, torch.transpose(data_X, 0, 1))\n",
    "        Psi = torch.matmul(data_Y, torch.transpose(data_Y, 0, 1))\n",
    "        Phi_tilde = Phi.fill_diagonal_(0.0)\n",
    "        Psi_tilde = Psi.fill_diagonal_(0.0)\n",
    "\n",
    "        one = torch.ones(n, 1).to(self.cuda_device)\n",
    "        oneT = torch.transpose(one, 0, 1)\n",
    "\n",
    "        PhiPsi = torch.matmul(Phi, Psi)\n",
    "        trPhiPsi = torch.trace(PhiPsi)\n",
    "        onePhiPsiOne = torch.matmul(oneT, torch.matmul(PhiPsi, one))\n",
    "\n",
    "        onePhione = torch.matmul(oneT, torch.matmul(Phi, one))\n",
    "        onePsione = torch.matmul(oneT, torch.matmul(Psi, one))\n",
    "        onePhioneonePsione = torch.matmul(onePhione, onePsione)\n",
    "\n",
    "        #Un = (\n",
    "        #   4 * (onePhioneonePsione - 4 * onePhiPsiOne + 2 * trPhiPsi)\n",
    "        # - 8 * (n-3) *(onePhiPsiOne - trPhiPsi)\n",
    "        # + 4 * (n-3)*(n-2) * trPhiPsi\n",
    "        # )\n",
    "\n",
    "        Un = (\n",
    "           4 * (onePhioneonePsione - 4 * onePhiPsiOne + 2 * trPhiPsi)\n",
    "         - 8 * (n-3) *(onePhiPsiOne - trPhiPsi)\n",
    "         - 8 * (n-3) *(onePhiPsiOne - trPhiPsi)\n",
    "\n",
    "\n",
    "         + 4 * (n-3)*(n-2) * trPhiPsi\n",
    "         )\n",
    "        \n",
    "        return(Un/n_four)\n",
    "\n",
    "    def u_stat_indep_matrix_efficient(self, data_X, data_Y):\n",
    "        #scalars\n",
    "        n = data_X.size(dim = 0)\n",
    "        \n",
    "        log_n_four = (\n",
    "        torch.log(torch.tensor(n))\n",
    "        +  \n",
    "        torch.log(torch.tensor(n-1))\n",
    "        +\n",
    "        torch.log(torch.tensor(n-2))\n",
    "        +\n",
    "        torch.log(torch.tensor(n-3))\n",
    "        )\n",
    "\n",
    "        #preliminary calculations\n",
    "        X_row_sum = torch.sum(data_X, axis = 0)\n",
    "        Y_row_sum = torch.sum(data_Y, axis = 0)\n",
    "        phi_psi = torch.einsum('ji,jk->ik', data_X, data_Y)\n",
    "        diag_Phi = torch.sum(torch.square(data_X), axis = 1)\n",
    "        diag_Psi = torch.sum(torch.square(data_Y), axis = 1)\n",
    "        rowsum_Phi = torch.einsum('i,ji -> j', X_row_sum, data_X)\n",
    "        rowsum_Psi = torch.einsum('ij, j -> i', data_Y, Y_row_sum)\n",
    "\n",
    "        #1. one term\n",
    "        one_Phi_one = torch.inner(X_row_sum, X_row_sum)\n",
    "        one_Psi_one = torch.inner(Y_row_sum, Y_row_sum)\n",
    "\n",
    "        tr_Phi = torch.sum(torch.square(data_X))\n",
    "        tr_Psi = torch.sum(torch.square(data_Y))\n",
    "\n",
    "        one_Phi_tilde_one = one_Phi_one - tr_Phi\n",
    "        one_Psi_tilde_one = one_Psi_one - tr_Psi\n",
    "\n",
    "        onePhioneonePsione = one_Phi_tilde_one * one_Psi_tilde_one\n",
    "\n",
    "\n",
    "        #2. one one term\n",
    "        onePhiPsiOne = torch.matmul(\n",
    "            torch.matmul(X_row_sum, phi_psi),\n",
    "            Y_row_sum)  + torch.inner(diag_Phi, diag_Psi)-torch.inner(rowsum_Phi, diag_Psi)-torch.inner(diag_Phi, rowsum_Psi)\n",
    "\n",
    "\n",
    "        #3. trace term\n",
    "        trPhiPsi = torch.sum( torch.square(phi_psi) ) - torch.inner(\n",
    "            torch.sum( torch.square(data_X), axis = 1),\n",
    "            torch.sum( torch.square(data_Y), axis = 1)\n",
    "        )\n",
    "        \n",
    "        sums = (4 * onePhioneonePsione - ( 8 * (n-1) ) * onePhiPsiOne + ( 4 * (n-1) * (n-2) ) * trPhiPsi )\n",
    "        \n",
    "        Un_sign = torch.sign(sums)\n",
    "        abs_Un = torch.exp(torch.log(torch.abs(sums)) - log_n_four)\n",
    "        Un = Un_sign * abs_Un\n",
    "\n",
    "        return(Un)\n",
    "    \n",
    "    def u_stat_indep_original(self, data_X, data_Y):\n",
    "        n = data_X.size(dim = 0)\n",
    "        print(f\"number of calculation = {2*scipy.special.comb(n,4) }\")\n",
    "        n_four = n * (n-1) * (n-2) * (n-3)\n",
    "        U_statistic = 0\n",
    "        for i in range(n):\n",
    "            set_j = set(range(n)) - {i}\n",
    "            for j in set_j:\n",
    "                set_k = set_j - {j}\n",
    "                for k in set_k:\n",
    "                    set_r = set_k - {k}\n",
    "                    for r in set_r:\n",
    "                        comb = [i,j,k,r]\n",
    "                        U_statistic = U_statistic + (\n",
    "                            self.kernel_indep(data_X[comb,]) * self.kernel_indep(data_Y[comb,])\n",
    "                        )/n_four\n",
    "        return(U_statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b08c3-fbf8-4bc7-852e-7a0f2f748d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d1c5f7e-f2ad-4c1d-90dc-2ec00cfff576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job started\n"
     ]
    }
   ],
   "source": [
    "print(\"job started\")\n",
    "import time\n",
    "import torch\n",
    "import sys\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "sys.path.append(\"/mnt/nas/users/mjm/GitHub/private_conti_test/Python_implm/modules\") \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8eabc936-d4f7-47ef-ab96-002a1191efd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "2\n",
      "code run on device:: cuda:0\n",
      "3072\n",
      "2097152\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available() \n",
    "print(f\"cuda available: {USE_CUDA}\")\n",
    "\n",
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(num_of_gpus)\n",
    "\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu') \n",
    "print(f\"code run on device:: {device}\")\n",
    "\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef8f4b25-200e-481e-8773-d29b541f139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = LDPIndepTester(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd094f9f-2b74-4362-a347-26f761d98b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####CHANGE HERE#####\n",
    "n = 5\n",
    "####################\n",
    "kappa = 2 #number of bins\n",
    "alpha = 1.2 #privacy level\n",
    "gamma = 0.05 # significance level\n",
    "nTests = 5 #number of tests for power estimation\n",
    "B = 3 # number of permutations\n",
    "d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4b203b4-3f0f-4467-9629-92bcbe8ecb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copula_mean\n",
      "tensor([0.5000, 0.5000], device='cuda:0')\n",
      "sigma\n",
      "tensor([[1.0000, 0.5000],\n",
      "        [0.5000, 1.0000]], device='cuda:0')\n",
      "\n",
      "1th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:0')\n",
      "noise dimension : 4, 4\n",
      "noise type: conti\n",
      "noise type: conti\n",
      "tensor([[ -3.0071,   0.4524, -39.9802,  32.2878],\n",
      "        [ 29.6390,  12.0011,  -2.4839,  17.6411],\n",
      "        [-10.4093,  22.8030, -19.2143, -21.2554],\n",
      "        [ -2.6714,  33.6989, -11.6111,  -8.6266],\n",
      "        [ 20.3033,  -1.7449,  -4.4924,   5.2683]], device='cuda:0')\n",
      "tensor([[ 42.5474,  11.6284,  -0.8522,  -5.5402],\n",
      "        [ -1.3604,  -3.7198,  10.0095,   4.3981],\n",
      "        [-16.1567,  -4.4040, -11.8561,   1.0391],\n",
      "        [  7.2874,  -1.1323,  -0.9694,  21.2928],\n",
      "        [ 35.3123, -14.4843,   6.7109, -21.2920]], device='cuda:0')\n",
      "original u-statistic:259529.15625\n",
      "perm_stat_now = 61678.265625\n",
      "perm_stat_now = -862394.5\n",
      "perm_stat_now = 544118.125\n",
      "\n",
      "2th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:0')\n",
      "noise dimension : 4, 4\n",
      "noise type: conti\n",
      "noise type: conti\n",
      "tensor([[-43.2905,  10.5492,  17.4626,  16.8794],\n",
      "        [  5.4020,  -4.1869,  -4.2439,   5.9512],\n",
      "        [ -8.5332,   8.1236, -42.6122,  15.0046],\n",
      "        [-17.1249,  21.2101, -21.6653, -34.4484],\n",
      "        [ 33.6324,  22.6911, -15.7912,   8.6413]], device='cuda:0')\n",
      "tensor([[ 15.4006, -17.5166, -11.0096,  13.5698],\n",
      "        [ 10.6738,   8.2372, -10.1950,  21.8419],\n",
      "        [  0.6871,   3.5789,   0.0323, -10.1834],\n",
      "        [ 25.2566,  -0.2846,   7.5042,   6.6729],\n",
      "        [ -0.3344,  -0.9413,  12.2914,  -9.7940]], device='cuda:0')\n",
      "original u-statistic:229024.921875\n",
      "perm_stat_now = -399162.65625\n",
      "perm_stat_now = 144295.234375\n",
      "perm_stat_now = 157940.796875\n",
      "\n",
      "3th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:0')\n",
      "noise dimension : 4, 4\n",
      "noise type: conti\n",
      "noise type: conti\n",
      "tensor([[ 27.1081,  13.6757, -45.5402,  -8.9780],\n",
      "        [ 34.2101, -32.5279,  -0.4076,  15.0504],\n",
      "        [ -0.9026,  -5.2555, -20.5111,  19.7696],\n",
      "        [ -8.3918,  -1.0623,  -9.4765,   9.3316],\n",
      "        [ 23.0427, -10.1095,  15.5847,  -4.5771]], device='cuda:0')\n",
      "tensor([[  2.6049,  15.4857,  23.1880, -12.2244],\n",
      "        [ -7.6214,   1.7906,  23.2701,   0.8707],\n",
      "        [-14.7785,   6.6134,  -4.2610,   3.7471],\n",
      "        [  5.7934,  -2.6246, -25.5475,   6.3064],\n",
      "        [-10.4927, -23.5348,   6.4833, -36.7389]], device='cuda:0')\n",
      "original u-statistic:170322.21875\n",
      "perm_stat_now = -30933.87890625\n",
      "perm_stat_now = 506422.75\n",
      "perm_stat_now = -111054.453125\n",
      "\n",
      "4th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:0')\n",
      "noise dimension : 4, 4\n",
      "noise type: conti\n",
      "noise type: conti\n",
      "tensor([[  7.4302, -29.2637,   2.3899,  -4.5622],\n",
      "        [ -1.0108,   3.5607, -15.7284,  -9.0474],\n",
      "        [ 17.1256, -11.8495,  -0.0396, -24.8454],\n",
      "        [-13.3027,  33.6326, -12.7691,  -1.0888],\n",
      "        [-10.8346, -13.9335, -10.9647, -12.9464]], device='cuda:0')\n",
      "tensor([[ 9.1023e+00, -2.3854e+01, -4.5543e+00, -4.8131e+01],\n",
      "        [ 1.0621e+00, -6.5672e+00, -1.2780e+01, -1.3401e+01],\n",
      "        [ 8.3931e+00, -5.9380e+00,  1.3040e+01, -5.4058e+00],\n",
      "        [ 3.6701e+01, -5.0023e+01,  2.8709e+01, -2.2346e+00],\n",
      "        [-2.5026e+01, -1.5612e+01, -8.1018e+00, -3.6050e-02]], device='cuda:0')\n",
      "original u-statistic:-765814.75\n",
      "perm_stat_now = -511736.9375\n",
      "perm_stat_now = 758264.0\n",
      "perm_stat_now = -279395.96875\n",
      "\n",
      "5th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:0')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:0')\n",
      "noise dimension : 4, 4\n",
      "noise type: conti\n",
      "noise type: conti\n",
      "tensor([[ -9.2887,   8.8206,  11.3458,   7.7060],\n",
      "        [ -3.9831,   0.9302,  13.1020, -15.9122],\n",
      "        [ -0.5799, -40.6484,  -8.6854,  34.4224],\n",
      "        [  7.5040,  -7.9679,  12.8615, -18.3324],\n",
      "        [ -5.4470,   1.9926,   0.9241,  22.4640]], device='cuda:0')\n",
      "tensor([[  6.6295,  13.9400,  13.7986,  -1.6341],\n",
      "        [  6.9475,  -4.9179, -10.6755,  19.2679],\n",
      "        [ 36.4869,  45.5488, -17.6568, -20.3157],\n",
      "        [ -1.5937,  -8.7303,  23.5611, -33.5370],\n",
      "        [ 28.0140,  -1.4490, -14.7113,   1.3245]], device='cuda:0')\n",
      "original u-statistic:253907.1875\n",
      "perm_stat_now = 853277.75\n",
      "perm_stat_now = 229654.828125\n",
      "perm_stat_now = 582943.5\n"
     ]
    }
   ],
   "source": [
    "multiplier = 1/2\n",
    "copula_mean = multiplier * torch.ones(d).to(device)\n",
    "\n",
    "\n",
    "\n",
    "sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(device)\n",
    "\n",
    "print(\"copula_mean\")\n",
    "print(copula_mean)\n",
    "\n",
    "\n",
    "\n",
    "print(\"sigma\")\n",
    "print(sigma)\n",
    "\n",
    "# set seed number for reproducibility\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "generator_Y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "    loc = copula_mean, \n",
    "    covariance_matrix = sigma)\n",
    "\n",
    "cdf_calculator = torch.distributions.normal.Normal(loc = 0.0, scale = 1.0)\n",
    "\n",
    "test_results = torch.empty(nTests)\n",
    "noise_vars_Y = torch.empty(nTests)\n",
    "noise_vars_Z = torch.empty(nTests)\n",
    "\n",
    "for rep in range(nTests):\n",
    "    print(f\"\\n{rep+1}th run\")\n",
    "    \n",
    "    #y_og = generator_Y.sample((n,))\n",
    "    y_og = torch.tensor(\n",
    "            [[0.9244, 0.5756],\n",
    "        [0.8182, 0.8254],\n",
    "        [0.5614, 0.7913],\n",
    "        [0.3196, 0.7090],\n",
    "        [0.3224, 0.4793]]).to(device)\n",
    "    data_y = cdf_calculator.cdf(y_og)   \n",
    "    data_z = cdf_calculator.cdf(-y_og)  \n",
    "    \n",
    "    print(data_y)\n",
    "    test_results[rep], noise_vars_Y[rep], noise_vars_Z[rep] = tester.run_test_conti_data(\n",
    "        B,\n",
    "        data_y,\n",
    "        data_z,\n",
    "        kappa, alpha, gamma,\n",
    "        #################\n",
    "        discrete_noise = False\n",
    "        ###############\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1015c7e9-4a30-40b8-a90b-b486d5143f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: 0.0\n",
      "power_upto_now: 0.0\n",
      "noise variance for Y: 410.0630187988281\n",
      "average noise variance for Y upto now: 359.712158203125\n",
      "noise variance for Z: 532.2941284179688\n",
      "average noise variance for Z upto now: 408.79498291015625\n",
      "power estimate : 0.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage noise variance for Z upto now: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(noise_vars_Z[:(rep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])\u001b[38;5;241m/\u001b[39m(rep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower estimate : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(test_results)\u001b[38;5;241m/\u001b[39mnTests \u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation ended at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    print(f\"result: {test_results[rep]}\")\n",
    "    print(f\"power_upto_now: { torch.sum(test_results[:(rep+1)])/(rep+1) }\")\n",
    "\n",
    "    print(f\"noise variance for Y: {noise_vars_Y[rep]}\")\n",
    "    print(f\"average noise variance for Y upto now: { torch.sum(noise_vars_Y[:(rep+1)])/(rep+1) }\")\n",
    "    \n",
    "    print(f\"noise variance for Z: {noise_vars_Z[rep]}\")\n",
    "    print(f\"average noise variance for Z upto now: { torch.sum(noise_vars_Z[:(rep+1)])/(rep+1) }\")\n",
    "  \n",
    "print( f\"power estimate : { torch.sum(test_results)/nTests }\" )\n",
    "print( f\"elapsed time: { time.time() - start_time }\" )\n",
    "print( f\"simulation ended at {datetime.datetime.now()}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423035a-2846-4932-8423-7fd1adabc190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
