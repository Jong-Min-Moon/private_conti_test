{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc1d945-5ed3-4ba2-8b6d-0b74d1f0ad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import time\n",
    "device2 = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fdc2ec4-f07b-42f9-93ea-a018417e3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDPTwoSampleTester:\n",
    "    def __init__(self, cuda_device):\n",
    "        self.cuda_device = cuda_device\n",
    "\n",
    "    def run_test_conti_data(self, B, data_X, data_Y, kappa, alpha, gamma, discrete = False):\n",
    "        '''\n",
    "        '''  \n",
    "        dataPrivatized = self.preprocess_conti_data(data_X, data_Y, kappa)\n",
    "        n_1 = data_X.size(dim = 0)\n",
    "        \n",
    " \n",
    "        \n",
    "        ustatOriginal = self.u_stat_twosample(dataPrivatized, n_1)\n",
    "        print(f\"original u-statistic:{ustatOriginal}\")\n",
    "        \n",
    "        #permutation procedure\n",
    "        permStats = torch.empty(B).to(self.cuda_device)\n",
    "        \n",
    "        for i in range(B):\n",
    "            perm_stat_now = self.u_stat_twosample(\n",
    "                dataPrivatized[torch.randperm(dataPrivatized.size(dim=0))],\n",
    "                n_1).to(self.cuda_device)\n",
    "            permStats[i] = perm_stat_now\n",
    "            #print(perm_stat_now)\n",
    "         \n",
    "        \n",
    "        p_value_proxy = (1 +\n",
    "                         torch.sum(\n",
    "                             torch.gt(input = permStats, other = ustatOriginal)\n",
    "                         )\n",
    "                        ) / (B + 1)\n",
    "        \n",
    "        \n",
    "        print(f\"p value proxy: {p_value_proxy}\")\n",
    "        return(p_value_proxy < gamma)#test result: TRUE = 1 = reject the null, FALSE = 0 = retain the null.\n",
    "    \n",
    "    def preprocess_conti_data(self, data_X, data_Y, kappa):\n",
    "        data_X_binned = self.bin(data_X, kappa)\n",
    "        data_Y_binned = self.bin(data_Y, kappa)\n",
    "        \n",
    "        dataCombined = torch.cat([data_X_binned, data_Y_binned], dim = 0)\n",
    "        dataPrivatized = self.privatize_twosample(dataCombined, alpha)\n",
    "        return(dataPrivatized)\n",
    "        \n",
    "\n",
    "    def bin(self, data, kappa): \n",
    "        ''' Only for continuous data'''\n",
    "        \n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "     \n",
    "        # 1. for each dimension, turn the continuous data into interval\n",
    "        # each row now indicates a hypercube in [0,1]^d\n",
    "        # the more the data is closer to 1, the larger the interval index.\n",
    "        dataInterval = self.transform_bin_index(data = data, nIntervals = kappa)\n",
    "        \n",
    "        # 2. for each datapoint(row),\n",
    "        #    turn the hypercube data into a multivariate data of (1, 2, ..., kappa^d)\n",
    "        #    each row now becomes an integer.\n",
    "        dataMultivariate = self.TransformMultivariate(\n",
    "            dataInterval = dataInterval,\n",
    "            nBin = kappa,\n",
    "        )\n",
    "        # 3. turn the indices into one-hot vectors\n",
    "        dataOnehot = self.TransformOnehot(dataMultivariate, kappa**d)\n",
    "        return(dataOnehot)\n",
    "    \n",
    "    def transform_bin_index(self, data, nIntervals):\n",
    "        ''' Only for continuous data.\n",
    "        for each dimension, transform the data in [0,1] into the interval index\n",
    "        first interval = [0, x], the others = (y z]\n",
    "        \n",
    "        input arguments\n",
    "            data: torch tensor object on GPU\n",
    "            nIntervals: integer\n",
    "        output\n",
    "            dataIndices: torch tensor, dimension same as the input\n",
    "        '''\n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "        breaks = torch.linspace(start = 0, end = 1, steps = nIntervals + 1).to(self.cuda_device) #floatTensor\n",
    "        dataIndices = torch.bucketize(data, breaks, right = False) # ( ] form.\n",
    "        dataIndices = dataIndices.add(\n",
    "            dataIndices.eq(0)\n",
    "        ) #move 0 values from the bin number 0 to the bin number 1        \n",
    "        return(dataIndices)\n",
    "    \n",
    "    def TransformMultivariate(self, dataInterval, nBin):\n",
    "        \"\"\"Only for continuous and multivariate data .\"\"\"\n",
    "        d = self.get_dimension(dataInterval)\n",
    "        \n",
    "        if d == 1:\n",
    "            return(dataInterval.sub(1))\n",
    "        else:\n",
    "            exponent = torch.linspace(start = (d-1), end = 0, steps = d, dtype = torch.long)\n",
    "            vector = torch.tensor(nBin).pow(exponent)\n",
    "            return( torch.matmul( dataInterval.sub(1).to(torch.float), vector.to(torch.float).to(device) ).to(torch.long) )\n",
    "    \n",
    "    def TransformOnehot(self, dataMultivariate, newdim):\n",
    "        return(\n",
    "            torch.nn.functional.one_hot(\n",
    "                dataMultivariate,\n",
    "                num_classes = newdim)\n",
    "        )\n",
    "    \n",
    "    def privatize_twosample(self, data, alpha = float(\"inf\"), discrete_noise = False):\n",
    "        ## assume the data is discrete by nature or has already been dicretized.\n",
    "        n = data.size(dim = 0)\n",
    "        dim = data.size(dim = 1) #kappa^d if conti data, d if discrete data\n",
    "        print(f\"noise dimension : {dim}\")\n",
    "        scale = torch.tensor(dim**(1/2))\n",
    "        \n",
    "        if alpha == float(\"inf\"): #non-private case\n",
    "            return( torch.mul(scale, data) )\n",
    "        else: # private case\n",
    "            if discrete_noise:\n",
    "                noise = self.noise_discrete(n = n, dim = dim, alpha = alpha)\n",
    "            else:\n",
    "                noise = self.noise_conti(n = n, dim = dim, alpha = alpha)\n",
    "        return(\n",
    "            \n",
    "            torch.add(\n",
    "                input = noise.reshape(n, -1),\n",
    "                alpha = scale,\n",
    "                other = data\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def noise_conti(self, n, dim, alpha):\n",
    "        #dim = kappa^d for conti data, d for discrete data\n",
    "        unit_laplace_generator = torch.distributions.laplace.Laplace(\n",
    "            torch.tensor(0.0).to(self.cuda_device),\n",
    "            torch.tensor(2**(-1/2)).to(self.cuda_device)\n",
    "        )\n",
    "        laplace_samples = unit_laplace_generator.sample(sample_shape = torch.Size([n * dim]))\n",
    "        scale = (8**(1/2) / alpha) * (dim**(1/2))\n",
    "        laplace_samples = scale*laplace_samples\n",
    "        print(f\"noise variance: {torch.var(laplace_samples)}\")\n",
    "        return(laplace_samples)\n",
    "    \n",
    "  \n",
    "        \n",
    "    \n",
    "    def noise_discrete(self, n, dim, alpha):\n",
    "        #dim = kappa^d for conti data, d for discrete data\n",
    "        param_geom = 1 - torch.exp(torch.tensor(-alpha / (2* (dim**(1/2)) )))\n",
    "        n_noise =  n * dim\n",
    "        geometric_generator = torch.distributions.geometric.Geometric(param_geom.to(self.cuda_device))\n",
    "        noise = geometric_generator.sample((n_noise,)) - geometric_generator.sample((n_noise,))\n",
    "        return(noise)\n",
    "    \n",
    "    def u_stat_twosample(self, data, n_1):\n",
    "        n_2 = data.size(dim = 0) - n_1\n",
    "        \n",
    "        data_x = data[ :n_1, ]\n",
    "        data_y = data[n_1: , ]\n",
    "        \n",
    "        # x only part\n",
    "        u_x = torch.matmul(data_x, torch.transpose(data_x, 0, 1))\n",
    "        u_x.fill_diagonal_(0)\n",
    "        u_x = torch.sum(u_x) / (n_1 * (n_1 - 1))\n",
    "        \n",
    "        # y only part\n",
    "        u_y = torch.matmul(data_y, torch.transpose(data_y, 0, 1))\n",
    "        u_y.fill_diagonal_(0)\n",
    "        u_y = torch.sum(u_y) / (n_2 * (n_2 - 1))\n",
    "\n",
    "        # x, y part\n",
    "        u_xy = torch.matmul(data_x, torch.transpose(data_y, 0, 1))\n",
    "        u_xy = torch.sum(u_xy) * ( 2 / (n_1 * n_2) )\n",
    "        return(u_x + u_y - u_xy)\n",
    "    \n",
    "    def get_dimension(self, data):\n",
    "        if data.dim() == 1:\n",
    "            return(1)\n",
    "        elif data.dim() == 2:\n",
    "            return( data.size(dim = 1) )\n",
    "        else:\n",
    "            return # we only use up to 2-dimensional tensor, i.e. matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8535df-7e76-41a5-b0a5-7dbc60bb336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####CHANGE HERE#####\n",
    "n1 = 20000\n",
    "n2 = 20000\n",
    "####################\n",
    "kappa = 5 #number of bins\n",
    "alpha = 0.3 #privacy level\n",
    "gamma = 0.05 # significance level\n",
    "nTests = 200 #number of tests for power estimation\n",
    "B = 200 # number of permutations\n",
    "\n",
    "start_time = time.time()\n",
    "tester2 = LDPTwoSampleTester(device2)\n",
    "copula_mean_1 = torch.tensor([-1.0, -1.0, -1.0]).to(device2)\n",
    "copula_mean_2 = torch.tensor([1.0, 1.0, 1.0]).to(device2)\n",
    "sigma = torch.tensor([[1.0, 0.5, 0.5], [0.5, 1.0, 0.5],  [0.5, 0.5, 1.0]]).to(device2)\n",
    "\n",
    "generator_X = torch.distributions.multivariate_normal.MultivariateNormal(loc = copula_mean_1, covariance_matrix = sigma)\n",
    "generator_Y = torch.distributions.multivariate_normal.MultivariateNormal(loc = copula_mean_2, covariance_matrix = sigma)\n",
    "cdf_calculator = torch.distributions.normal.Normal(loc = 0.0, scale = 1.0)\n",
    "\n",
    "test_results = torch.empty(nTests)\n",
    "for rep in range(nTests):\n",
    "    print(f\"{rep+1}th run\")\n",
    "    \n",
    "    data_x = cdf_calculator.cdf(generator_X.sample((n1,)))\n",
    "    data_y = cdf_calculator.cdf(generator_Y.sample((n2,)))\n",
    "    \n",
    "    \n",
    "    result_now = tester2.run_test_conti_data(B, data_x, data_y,\n",
    "                                            kappa, alpha, gamma, discrete = False\n",
    "                                           )\n",
    "    test_results[rep] = result_now\n",
    "    print(f\"result: {result_now}\")\n",
    "  \n",
    "print( f\"power estimate : { torch.sum(test_results)/nTests }\" )\n",
    "print( f\"elapsed time: { time.time() - start_time }\" )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
