{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "469fbe7b-78a6-466e-ab3a-79ab17cf9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from private_me.data import TSTData\n",
    "#import private_me.util as util\n",
    "#import private_me.kernel as kernel\n",
    "#from private_me.private_mechanism import gauss_mech, improve_gauss_mech, analyse_gauss_mech\n",
    "#from scipy.linalg import block_diag, sqrtm, inv, svd\n",
    "\n",
    "\n",
    "from scipy.linalg import block_diag, sqrtm, inv, svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7e00b4-9f7d-47ef-a1cd-5596e448787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tester(object):\n",
    "    \"\"\"Abstract class for two sample tests.\"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, gamma, cuda_device, seed):\n",
    "        \"\"\"\n",
    "        gamma: significance level of the test\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.cuda_device = cuda_device\n",
    "        self.seed = seed\n",
    "    \n",
    "\n",
    "    \n",
    "    @abstractmethod\n",
    "    def permu_test(self):\n",
    "        \"\"\"perform the two-sample test and return values computed in a dictionary:\n",
    "        {alpha: 0.01, pvalue: 0.0002, test_stat: 2.3, h0_rejected: True, ...}\n",
    "        tst_data: an instance of TSTData\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_stat(self):\n",
    "        \"\"\"Compute the test statistic\"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def privatize(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def LapU(self, oneHot, alpha, c, theta):\n",
    "        ''' Only for continuous data.\n",
    "        for each dimension, transform the data in [0,1] into the interval index\n",
    "        first interval = [0, x], the others = (y z]\n",
    "        \n",
    "        input arguments\n",
    "            data: torch tensor object on GPU of multivariate data\n",
    "            d: number of categories of multivariate data\n",
    "            alpha: privacy level\n",
    "            c: noise scale paramter\n",
    "        output\n",
    "            LDPView: \\alpha-LDP view of the input multivariate data\n",
    "        '''\n",
    " \n",
    "        sigma = c * 2**(1/2) * theta / alpha\n",
    "        laplaceSize = oneHot.size()\n",
    "        laplaceNoise = self.generate_unit_laplace(laplaceSize)\n",
    "        LDPView = torch.tensor(theta) * oneHot + sigma * laplaceNoise\n",
    "        return(LDPView)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def h_bin(self, data, kappa): \n",
    "        ''' Only for continuous data\n",
    "        input arguments\n",
    "            data: torch tensor of continuous data\n",
    "            kappa: number of bin in each dimension\n",
    "        output\n",
    "            torch tensor of multivariate data\n",
    "        '''\n",
    "               \n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "     \n",
    "        # 1. for each dimension, turn the continuous data into interval\n",
    "        # each row now indicates a hypercube in [0,1]^d\n",
    "        # the more the data is closer to 1, the larger the interval index.\n",
    "        dataBinIndex = self.transform_bin_index(data = data, nIntervals = kappa)\n",
    "        \n",
    "        # 2. for each datapoint(row),\n",
    "        #    turn the hypercube data into a multivariate data of (1, 2, ..., kappa^d)\n",
    "        #    each row now becomes an integer.\n",
    "        dataMultivariate = self.TransformMultivariate(dataBinIndex, kappa)\n",
    "        print(dataMultivariate)\n",
    "        return(dataMultivariate)\n",
    "    \n",
    "    def transform_bin_index(self, data, nIntervals):\n",
    "        ''' Only for continuous data.\n",
    "        for each dimension, transform the data in [0,1] into the interval index\n",
    "        first interval = [0, x], the others = (y z]\n",
    "        \n",
    "        input arguments\n",
    "            data: torch tensor object on GPU\n",
    "            nIntervals: integer\n",
    "        output\n",
    "            dataIndices: torch tensor, dimension same as the input\n",
    "        '''\n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "        breaks = torch.linspace(start = 0, end = 1, steps = nIntervals + 1).to(self.cuda_device) #floatTensor\n",
    "        dataIndices = torch.bucketize(data, breaks, right = False) # ( ] form.\n",
    "        dataIndices = dataIndices.add(\n",
    "            dataIndices.eq(0)\n",
    "        ) #move 0 values from the bin number 0 to the bin number 1        \n",
    "        return(dataIndices)    \n",
    "\n",
    "    def TransformMultivariate(self, dataBinIndex, nBin):\n",
    "        \"\"\"Only for continuous and multivariate data .\"\"\"\n",
    "        d = self.get_dimension(dataBinIndex)\n",
    "        \n",
    "        if d == 1:\n",
    "            return(dataInterval.sub(1))\n",
    "        else:\n",
    "            exponent = torch.linspace(start = (d-1), end = 0, steps = d, dtype = torch.long)\n",
    "            vector = torch.tensor(nBin).pow(exponent)\n",
    "            return( torch.matmul( dataBinIndex.sub(1).to(torch.float), vector.to(torch.float).to(self.cuda_device) ).to(torch.long) )\n",
    "    \n",
    "    \n",
    "    def generate_unit_laplace(self, size):\n",
    "        '''\n",
    "        input: torch.size object\n",
    "        output: torch tensor of data from unit laplace distribution\n",
    "        '''\n",
    "     \n",
    "        unit_laplace_generator = torch.distributions.laplace.Laplace(\n",
    "            torch.tensor(0.0).to(self.cuda_device),\n",
    "            torch.tensor(2**(-1/2)).to(self.cuda_device)\n",
    "        )\n",
    "        return unit_laplace_generator.sample(sample_shape = size)\n",
    "        \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_onehot(dataMultivariate, d):\n",
    "        return(\n",
    "            torch.nn.functional.one_hot(\n",
    "                dataMultivariate,\n",
    "                num_classes = d)\n",
    "        )\n",
    " \n",
    "\n",
    "    @staticmethod\n",
    "    def get_dimension(data):\n",
    "        if data.dim() == 1:\n",
    "            return(1)\n",
    "        elif data.dim() == 2:\n",
    "            return( data.size(dim = 1) )\n",
    "        else:\n",
    "            return # we only use up to 2-dimensional tensor, i.e. matrix\n",
    "\n",
    "    @staticmethod        \n",
    "    def range_check(self, data):\n",
    "        if (torch.sum(data.gt(1))).gt(0):\n",
    "            print(\"check data range\")\n",
    "            return False\n",
    "        elif (torch.sum(data.lt(0))).gt(0):\n",
    "            print(\"check data range\")\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4de135c-51fe-4d7c-857f-34f6a8a8384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(object):\n",
    "    \"\"\"Abstract class for two sample tests.\"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, cuda_device, seed):\n",
    "        self.cuda_device = cuda_device\n",
    "        self.seed = seed\n",
    "        self.cdf_calculator = torch.distributions.normal.Normal(loc = 0.0, scale = 1.0)\n",
    "    \n",
    "   # @abstractmethod   \n",
    "    #def generate_y(self):\n",
    "        #raise NotImplementedError(\"implement generate_y\")\n",
    "        \n",
    "    @abstractmethod   \n",
    "    def generate_z(self):\n",
    "        raise NotImplementedError(\"implement generate_z\")\n",
    "        \n",
    "    def calculate_cdf(self, data):\n",
    "        return self.cdf_calculator.cdf(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9f6a5d0-cd07-4e39-b741-95e966159a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoSampleContiTester(tester):\n",
    "    def __init__(self, gamma, cuda_device, seed, kappa):\n",
    "        super(twoSampleContiTester, self).__init__(gamma, cuda_device, seed)\n",
    "        self.kappa = kappa\n",
    "    \n",
    "    def estimate_power(self, data_generator, alpha, B, n_test):\n",
    "        torch.manual_seed(0)\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        start_time = time.time()\n",
    "        print(f\"\"\"\n",
    "        simulation started at = {datetime.datetime.now()} \\n\n",
    "        n1 = {data_generator.n1}, n2 = {data_generator.n2}, \\n\n",
    "        kappa = {self.kappa}, alpha = {alpha},\\n\n",
    "        gamma = {self.gamma}, nTests = {n_test},\\n\n",
    "        B = {B}, d = {data_generator.d}\n",
    "        \"\"\")\n",
    "        test_results = torch.empty(n_test)\n",
    "        \n",
    "        for rep in range(n_test):\n",
    "            print(f\"\\n{rep+1}th run\")\n",
    "            tst_data_y = data_generator.generate_y()\n",
    "            tst_data_z = data_generator.generate_z()\n",
    "            test_results[rep] = self.permu_test(tst_data_y, tst_data_z, alpha, B)\n",
    "            print(f\"result: {test_results[rep]}\")\n",
    "            print(f\"power_upto_now: { torch.sum(test_results[:(rep+1)])/(rep+1) }\")\n",
    "  \n",
    "        print( f\"power estimate : { torch.sum(test_results)/n_test }\" )\n",
    "        print( f\"elapsed time: { time.time() - start_time }\" )\n",
    "        print( f\"simulation ended at {datetime.datetime.now()}\" )\n",
    "        return(torch.sum(test_results)/n_test)\n",
    "        \n",
    "    def permu_test(self, tst_data_y, tst_data_z, alpha, B): \n",
    "        n_1 = tst_data_y.size(dim = 0)\n",
    "        tst_data_priv = self.privatize(tst_data_y, tst_data_z, alpha)\n",
    "        n = tst_data_priv.size(dim = 0)\n",
    "        \n",
    "        #original statistic\n",
    "        ustatOriginal = self.compute_stat(tst_data_priv[:n_1,:], tst_data_priv[n_1:,:])\n",
    "        print(f\"original u-statistic:{ustatOriginal}\")\n",
    "        \n",
    "        #permutation procedure\n",
    "        permStats = torch.empty(B).to(self.cuda_device)\n",
    "        \n",
    "        for i in range(B):\n",
    "            permutation = torch.randperm(n)\n",
    "            perm_stat_now = self.compute_stat(\n",
    "                tst_data_priv[permutation][:n_1,:],\n",
    "                tst_data_priv[permutation][n_1:,:]\n",
    "            ).to(self.cuda_device)\n",
    "            permStats[i] = perm_stat_now\n",
    "\n",
    "               \n",
    "        p_value_proxy = (1 +\n",
    "                         torch.sum(\n",
    "                             torch.gt(input = permStats, other = ustatOriginal)\n",
    "                         )\n",
    "                        ) / (B + 1)\n",
    "      \n",
    "        print(f\"p value proxy: {p_value_proxy}\")\n",
    "        return(p_value_proxy < self.gamma)#test result: TRUE = 1 = reject the null, FALSE = 0 = retain the null.    \n",
    " \n",
    "    def compute_stat(self, tst_data_y_priv, tst_data_z_priv):\n",
    "        n_1 = torch.tensor(tst_data_y_priv.size(dim = 0))\n",
    "        n_2 = torch.tensor(tst_data_z_priv.size(dim = 0))\n",
    "    \n",
    "        y_row_sum = torch.sum(tst_data_y_priv, axis = 0)\n",
    "        z_row_sum = torch.sum(tst_data_z_priv, axis = 0)\n",
    "        phi_psi = torch.einsum('ji,jk->ik', tst_data_y_priv, tst_data_z_priv)\n",
    "\n",
    "\n",
    "        one_Phi_one = torch.inner(y_row_sum, y_row_sum)\n",
    "        one_Psi_one = torch.inner(z_row_sum, z_row_sum)\n",
    "\n",
    "        tr_Phi = torch.sum(torch.square(tst_data_y_priv))\n",
    "        tr_Psi = torch.sum(torch.square(tst_data_z_priv))\n",
    "\n",
    "        one_Phi_tilde_one = one_Phi_one - tr_Phi\n",
    "        one_Psi_tilde_one = one_Psi_one - tr_Psi\n",
    "\n",
    "        onePhioneonePsione = one_Phi_tilde_one * one_Psi_tilde_one\n",
    "\n",
    "        # y only part. log calculation in case of large n1\n",
    "        sign_y = torch.sign(one_Phi_tilde_one)\n",
    "        abs_u_y = torch.exp(torch.log(torch.abs(one_Phi_tilde_one)) - torch.log(n_1) - torch.log(n_1 - 1) )\n",
    "        u_y = sign_y * abs_u_y\n",
    "\n",
    "\n",
    "        # z only part. log calculation in case of large n2\n",
    "        sign_z = torch.sign(one_Psi_tilde_one)\n",
    "\n",
    "        abs_u_z = torch.exp(torch.log(torch.abs(one_Psi_tilde_one)) - torch.log(n_2) - torch.log(n_2 - 1) )\n",
    "        u_z = sign_z * abs_u_z\n",
    "\n",
    "        # cross part\n",
    "        cross = torch.inner(y_row_sum, z_row_sum)\n",
    "        sign_cross = torch.sign(cross)\n",
    "        abs_cross = torch.exp(torch.log(torch.abs(cross)) +torch.log(torch.tensor(2))- torch.log(n_1) - torch.log(n_2) )\n",
    "        u_cross = sign_cross * abs_cross\n",
    "\n",
    "        return(u_y + u_z - u_cross)\n",
    "    \n",
    "        \n",
    "    def privatize(self, tst_data_y, tst_data_z, alpha):\n",
    "        d = self.kappa ** tst_data_y.size(dim = 1)\n",
    "        theta = d**(1/2)\n",
    "        tst_data_y_multi = self.h_bin(tst_data_y, self.kappa)\n",
    "        tst_data_y_oneHot = self.transform_onehot(tst_data_y_multi, d)\n",
    "        tst_data_z_multi = self.h_bin(tst_data_z, self.kappa) \n",
    "        tst_data_z_oneHot = self.transform_onehot(tst_data_z_multi, d)\n",
    "        dataCombined = torch.cat([tst_data_y_oneHot, tst_data_z_oneHot], dim = 0)\n",
    "        tst_data_priv = self.LapU(dataCombined, alpha, 2, theta)\n",
    "        return(tst_data_priv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5a39186-97f9-4c19-949c-0e19b253e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndepContiTester(tester):\n",
    "    def __init__(self, gamma, cuda_device, seed, kappa):\n",
    "        super(IndepContiTester, self).__init__(gamma, cuda_device, seed)\n",
    "        self.kappa = kappa\n",
    "    \n",
    "    def estimate_power(self, data_generator, alpha, B, n_test):\n",
    "        torch.manual_seed(0)\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        start_time = time.time()\n",
    "        print(f\"\"\"\n",
    "        simulation started at = {datetime.datetime.now()} \\n\n",
    "        n = {data_generator.n}, \\n\n",
    "        kappa = {self.kappa}, alpha = {alpha},\\n\n",
    "        gamma = {self.gamma}, nTests = {n_test},\\n\n",
    "        B = {B}, d = {data_generator.d}\n",
    "        \"\"\")\n",
    "        \n",
    "        test_results = torch.empty(n_test)\n",
    "        \n",
    "        for rep in range(n_test):\n",
    "            print(f\"\\n{rep+1}th run\")\n",
    "            tst_data_y = data_generator.generate_y()\n",
    "            tst_data_z = data_generator.generate_z()\n",
    "            test_results[rep] = self.permu_test(tst_data_y, tst_data_z, alpha, B)\n",
    "            print(f\"result: {test_results[rep]}\")\n",
    "            print(f\"power_upto_now: { torch.sum(test_results[:(rep+1)])/(rep+1) }\")\n",
    "  \n",
    "        print( f\"power estimate : { torch.sum(test_results)/n_test }\" )\n",
    "        print( f\"elapsed time: { time.time() - start_time }\" )\n",
    "        print( f\"simulation ended at {datetime.datetime.now()}\" )\n",
    "        return(torch.sum(test_results)/n_test)\n",
    "\n",
    "    #done\n",
    "    def permu_test(self, tst_data_y, tst_data_z, alpha, B): \n",
    "        n = tst_data_z.size(dim = 0)\n",
    "        tst_data_priv_y, tst_data_priv_z = self.privatize(tst_data_y, tst_data_z, alpha)\n",
    "        print(tst_data_priv_y)\n",
    "        print(tst_data_priv_z)\n",
    "        \n",
    "        #original statistic\n",
    "        ustatOriginal = self.compute_stat(tst_data_priv_y, tst_data_priv_z)\n",
    "        print(f\"original u-statistic:{ustatOriginal}\")\n",
    "        \n",
    "        #permutation procedure\n",
    "        permStats = torch.empty(B).to(self.cuda_device)\n",
    "        \n",
    "        for i in range(B):\n",
    "            permutation = torch.randperm(n)\n",
    "            perm_stat_now = self.compute_stat(\n",
    "                tst_data_priv_y,\n",
    "                tst_data_priv_z[permutation]\n",
    "            ).to(self.cuda_device)\n",
    "            permStats[i] = perm_stat_now\n",
    "\n",
    "               \n",
    "        p_value_proxy = (1 +\n",
    "                         torch.sum(\n",
    "                             torch.gt(input = permStats, other = ustatOriginal)\n",
    "                         )\n",
    "                        ) / (B + 1)\n",
    "      \n",
    "        print(f\"p value proxy: {p_value_proxy}\")\n",
    "        return(p_value_proxy < self.gamma)#test result: TRUE = 1 = reject the null, FALSE = 0 = retain the null.    \n",
    "\n",
    "    #done\n",
    "    def compute_stat(self, tst_data_y_priv, tst_data_z_priv):\n",
    "        #scalars\n",
    "        n = tst_data_y_priv.size(dim = 0)\n",
    "        \n",
    "        log_n_four = (\n",
    "        torch.log(torch.tensor(n))\n",
    "        +  \n",
    "        torch.log(torch.tensor(n-1))\n",
    "        +\n",
    "        torch.log(torch.tensor(n-2))\n",
    "        +\n",
    "        torch.log(torch.tensor(n-3))\n",
    "        )\n",
    "\n",
    "        #preliminary calculations\n",
    "        y_row_sum = torch.sum(tst_data_y_priv, axis = 0)\n",
    "        z_row_sum = torch.sum(tst_data_z_priv, axis = 0)\n",
    "        phi_psi = torch.einsum('ji,jk->ik', tst_data_y_priv, tst_data_z_priv)\n",
    "        diag_Phi = torch.sum(torch.square(tst_data_y_priv), axis = 1)\n",
    "        diag_Psi = torch.sum(torch.square(tst_data_z_priv), axis = 1)\n",
    "        rowsum_Phi = torch.einsum('i,ji -> j', y_row_sum, tst_data_y_priv)\n",
    "        rowsum_Psi = torch.einsum('ij, j -> i', tst_data_z_priv, z_row_sum)\n",
    "\n",
    "        #1. one term\n",
    "        one_Phi_one = torch.inner(y_row_sum, y_row_sum)\n",
    "        one_Psi_one = torch.inner(z_row_sum, z_row_sum)\n",
    "\n",
    "        tr_Phi = torch.sum(torch.square(tst_data_y_priv))\n",
    "        tr_Psi = torch.sum(torch.square(tst_data_z_priv))\n",
    "\n",
    "        one_Phi_tilde_one = one_Phi_one - tr_Phi\n",
    "        one_Psi_tilde_one = one_Psi_one - tr_Psi\n",
    "\n",
    "        onePhioneonePsione = one_Phi_tilde_one * one_Psi_tilde_one\n",
    "\n",
    "\n",
    "        #2. one one term\n",
    "        onePhiPsiOne = torch.matmul(\n",
    "            torch.matmul(y_row_sum, phi_psi),\n",
    "            z_row_sum)  + torch.inner(diag_Phi, diag_Psi)-torch.inner(rowsum_Phi, diag_Psi)-torch.inner(diag_Phi, rowsum_Psi)\n",
    "\n",
    "\n",
    "        #3. trace term\n",
    "        trPhiPsi = torch.sum( torch.square(phi_psi) ) - torch.inner(\n",
    "            torch.sum( torch.square(tst_data_y_priv), axis = 1),\n",
    "            torch.sum( torch.square(tst_data_z_priv), axis = 1)\n",
    "        )\n",
    "        \n",
    "        sums = (4 * onePhioneonePsione - ( 8 * (n-1) ) * onePhiPsiOne + ( 4 * (n-1) * (n-2) ) * trPhiPsi )\n",
    "        \n",
    "        Un_sign = torch.sign(sums)\n",
    "        abs_Un = torch.exp(torch.log(torch.abs(sums)) - log_n_four)\n",
    "        Un = Un_sign * abs_Un\n",
    "\n",
    "        return(Un)\n",
    "    \n",
    "      #done  \n",
    "    def privatize(self, tst_data_y, tst_data_z, alpha):\n",
    "        d1 = self.kappa ** tst_data_y.size(dim = 1)\n",
    "        d2 = self.kappa ** tst_data_z.size(dim = 1)\n",
    "        theta = (d1*d2)**(1/2)\n",
    "        print(tst_data_y)\n",
    "        tst_data_y_multi = self.h_bin(tst_data_y, self.kappa)\n",
    "        tst_data_z_multi = self.h_bin(tst_data_z, self.kappa) \n",
    "        \n",
    "        \n",
    "        tst_data_y_oneHot = self.transform_onehot(tst_data_y_multi, d1)\n",
    "        print(tst_data_y_oneHot)\n",
    "        tst_data_z_oneHot = self.transform_onehot(tst_data_z_multi, d2)\n",
    "\n",
    "        tst_data_priv_y = self.LapU(tst_data_y_oneHot, alpha, 4, theta)\n",
    "        tst_data_priv_z = self.LapU(tst_data_z_oneHot, alpha, 4, theta)\n",
    "        return(tst_data_priv_y, tst_data_priv_z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a0f86c-7e08-4a4a-9e39-1b7f27346dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_sample_generator_mean_departure(data_generator):\n",
    "    def __init__(self, cuda_device, seed, n1, n2, d):\n",
    "        super(two_sample_generator_mean_departure, self).__init__(cuda_device, seed)\n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "        self.d = d\n",
    "\n",
    "        copula_mean_y = -1/2 * torch.ones(d).to(self.cuda_device)\n",
    "        copula_mean_z =  1/2 * torch.ones(d).to(self.cuda_device)\n",
    "\n",
    "        sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(self.cuda_device)\n",
    "\n",
    "\n",
    "        print(\"copula_mean_y\")\n",
    "        print(copula_mean_y)\n",
    "\n",
    "        print(\"copula_mean_z\")\n",
    "        print(copula_mean_z)\n",
    "\n",
    "        print(\"sigma\")\n",
    "        print(sigma)\n",
    "\n",
    "        self.generator_y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean_y, \n",
    "            covariance_matrix = sigma)\n",
    "        self.generator_z = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean_z,\n",
    "            covariance_matrix = sigma)\n",
    "        \n",
    "    def generate_y(self):\n",
    "            normalSample = self.generator_y.sample( (self.n1,) )\n",
    "            return( self.calculate_cdf(normalSample) )  \n",
    "        \n",
    "    def generate_z(self):\n",
    "            return(\n",
    "                self.calculate_cdf(\n",
    "                    self.generator_z.sample( (self.n2,) )\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44de59b8-5c3f-4d51-8e82-70d1dbc47724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class indep_generator_trivial(data_generator):\n",
    "    def __init__(self, cuda_device, seed, n, d):\n",
    "        super(indep_generator_trivial, self).__init__(cuda_device, seed)\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.normalsample = 0\n",
    "        copula_mean = -1/2 * torch.ones(d).to(self.cuda_device)\n",
    "\n",
    "        sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(self.cuda_device)\n",
    "\n",
    "\n",
    "        print(\"copula_mean\")\n",
    "        print(copula_mean)\n",
    "\n",
    "\n",
    "        print(\"sigma\")\n",
    "        print(sigma)\n",
    "\n",
    "        self.generator_y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean, \n",
    "            covariance_matrix = sigma)\n",
    "\n",
    "        \n",
    "    def generate_y(self):\n",
    "            #self.normalSample = self.generator_y.sample( (self.n,) )\n",
    "            self.normalSample = torch.tensor(\n",
    "            [[0.9244, 0.5756],\n",
    "        [0.8182, 0.8254],\n",
    "        [0.5614, 0.7913],\n",
    "        [0.3196, 0.7090],\n",
    "        [0.3224, 0.4793]]\n",
    "            ).to(self.cuda_device)\n",
    "            return( self.calculate_cdf(self.normalSample) )  \n",
    "        \n",
    "    def generate_z(self):\n",
    "            return(\n",
    "                self.calculate_cdf(\n",
    "                    -self.normalSample\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe57dba-70b4-4d83-b523-a4b71abd09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class indep_generator_nontrivial(data_generator):\n",
    "    def __init__(self, cuda_device, seed, n, d):\n",
    "        super(indep_generator_trivial, self).__init__(cuda_device, seed)\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.normalsample = 0\n",
    "        copula_mean = -1/2 * torch.ones(d).to(self.cuda_device)\n",
    "\n",
    "        sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(self.cuda_device)\n",
    "\n",
    "\n",
    "        print(\"copula_mean\")\n",
    "        print(copula_mean)\n",
    "\n",
    "\n",
    "        print(\"sigma\")\n",
    "        print(sigma)\n",
    "\n",
    "        self.generator_y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean, \n",
    "            covariance_matrix = sigma)\n",
    "\n",
    "        \n",
    "    def generate_y(self):\n",
    "        self.normalSample = self.generator_y.sample( (self.n,) )\n",
    "        return( self.calculate_cdf(self.normalSample) )  \n",
    "        \n",
    "    def generate_z(self):\n",
    "        return(\n",
    "            self.calculate_cdf(\n",
    "                -self.normalSample\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4288a19c-be43-49cc-ac06-b331265c5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class indep_generator_nontrivial(data_generator):\n",
    "    def __init__(self, cuda_device, seed, n, d):\n",
    "        super(indep_generator_trivial, self).__init__(cuda_device, seed)\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.normalsample = 0\n",
    "        copula_mean = -1/2 * torch.ones(d).to(self.cuda_device)\n",
    "\n",
    "        sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(self.cuda_device)\n",
    "\n",
    "\n",
    "        print(\"copula_mean\")\n",
    "        print(copula_mean)\n",
    "\n",
    "\n",
    "        print(\"sigma\")\n",
    "        print(sigma)\n",
    "\n",
    "        self.generator_y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean, \n",
    "            covariance_matrix = sigma)\n",
    "\n",
    "        \n",
    "    def generate_y(self):\n",
    "        self.normalSample = self.generator_y.sample( (self.n,) )\n",
    "        return( self.calculate_cdf(self.normalSample) )  \n",
    "        \n",
    "    def generate_z(self):\n",
    "        return(\n",
    "            self.calculate_cdf(\n",
    "                torch.sin(self.normalSample) + 0.05\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c55db41-9fa2-4ebe-9c5f-cf82cea111f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "code run on device:: cuda:1\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available() \n",
    "print(f\"cuda available: {USE_CUDA}\")\n",
    "\n",
    "device = torch.device('cuda:1' if USE_CUDA else 'cpu') \n",
    "print(f\"code run on device:: {device}\")\n",
    "tester = IndepContiTester(gamma = 0.05, cuda_device = device, seed = 0, kappa = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64ce2021-2c39-45c5-9916-62761c1d8336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copula_mean\n",
      "tensor([-0.5000, -0.5000], device='cuda:1')\n",
      "sigma\n",
      "tensor([[1.0000, 0.5000],\n",
      "        [0.5000, 1.0000]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "generator = indep_generator_trivial(cuda_device = device, seed = 0, n = 5, d = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9147f35-e0f2-465d-abe6-bf312295ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        simulation started at = 2022-09-05 20:14:45.887918 \n",
      "\n",
      "        n = 5, \n",
      "\n",
      "        kappa = 2, alpha = 1.2,\n",
      "\n",
      "        gamma = 0.05, nTests = 5,\n",
      "\n",
      "        B = 3, d = 2\n",
      "        \n",
      "\n",
      "1th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:1')\n",
      "tensor([3, 3, 3, 3, 3], device='cuda:1')\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:1')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:1')\n",
      "tensor([[ -3.0071,   0.4524, -39.9802,  32.2878],\n",
      "        [ 29.6390,  12.0011,  -2.4839,  17.6411],\n",
      "        [-10.4093,  22.8030, -19.2143, -21.2554],\n",
      "        [ -2.6714,  33.6989, -11.6111,  -8.6266],\n",
      "        [ 20.3033,  -1.7449,  -4.4924,   5.2683]], device='cuda:1')\n",
      "tensor([[ 42.5474,  11.6284,  -0.8522,  -5.5402],\n",
      "        [ -1.3604,  -3.7198,  10.0095,   4.3981],\n",
      "        [-16.1567,  -4.4040, -11.8561,   1.0391],\n",
      "        [  7.2874,  -1.1323,  -0.9694,  21.2928],\n",
      "        [ 35.3123, -14.4843,   6.7109, -21.2920]], device='cuda:1')\n",
      "original u-statistic:259529.640625\n",
      "p value proxy: 0.5\n",
      "result: 0.0\n",
      "power_upto_now: 0.0\n",
      "\n",
      "2th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:1')\n",
      "tensor([3, 3, 3, 3, 3], device='cuda:1')\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:1')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:1')\n",
      "tensor([[-43.2905,  10.5492,  17.4626,  16.8794],\n",
      "        [  5.4020,  -4.1869,  -4.2439,   5.9512],\n",
      "        [ -8.5332,   8.1236, -42.6122,  15.0046],\n",
      "        [-17.1249,  21.2101, -21.6653, -34.4484],\n",
      "        [ 33.6324,  22.6911, -15.7912,   8.6413]], device='cuda:1')\n",
      "tensor([[ 15.4006, -17.5166, -11.0096,  13.5698],\n",
      "        [ 10.6738,   8.2372, -10.1950,  21.8419],\n",
      "        [  0.6871,   3.5789,   0.0323, -10.1834],\n",
      "        [ 25.2566,  -0.2846,   7.5042,   6.6729],\n",
      "        [ -0.3344,  -0.9413,  12.2914,  -9.7940]], device='cuda:1')\n",
      "original u-statistic:229025.375\n",
      "p value proxy: 0.25\n",
      "result: 0.0\n",
      "power_upto_now: 0.0\n",
      "\n",
      "3th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:1')\n",
      "tensor([3, 3, 3, 3, 3], device='cuda:1')\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:1')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:1')\n",
      "tensor([[ 27.1081,  13.6757, -45.5402,  -8.9780],\n",
      "        [ 34.2101, -32.5279,  -0.4076,  15.0504],\n",
      "        [ -0.9026,  -5.2555, -20.5111,  19.7696],\n",
      "        [ -8.3918,  -1.0623,  -9.4765,   9.3316],\n",
      "        [ 23.0427, -10.1095,  15.5847,  -4.5771]], device='cuda:1')\n",
      "tensor([[  2.6049,  15.4857,  23.1880, -12.2244],\n",
      "        [ -7.6214,   1.7906,  23.2701,   0.8707],\n",
      "        [-14.7785,   6.6134,  -4.2610,   3.7471],\n",
      "        [  5.7934,  -2.6246, -25.5475,   6.3064],\n",
      "        [-10.4927, -23.5348,   6.4833, -36.7389]], device='cuda:1')\n",
      "original u-statistic:170322.546875\n",
      "p value proxy: 0.5\n",
      "result: 0.0\n",
      "power_upto_now: 0.0\n",
      "\n",
      "4th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:1')\n",
      "tensor([3, 3, 3, 3, 3], device='cuda:1')\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:1')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:1')\n",
      "tensor([[  7.4302, -29.2637,   2.3899,  -4.5622],\n",
      "        [ -1.0108,   3.5607, -15.7284,  -9.0474],\n",
      "        [ 17.1256, -11.8495,  -0.0396, -24.8454],\n",
      "        [-13.3027,  33.6326, -12.7691,  -1.0888],\n",
      "        [-10.8346, -13.9335, -10.9647, -12.9464]], device='cuda:1')\n",
      "tensor([[ 9.1023e+00, -2.3854e+01, -4.5543e+00, -4.8131e+01],\n",
      "        [ 1.0621e+00, -6.5672e+00, -1.2780e+01, -1.3401e+01],\n",
      "        [ 8.3931e+00, -5.9380e+00,  1.3040e+01, -5.4058e+00],\n",
      "        [ 3.6701e+01, -5.0023e+01,  2.8709e+01, -2.2346e+00],\n",
      "        [-2.5026e+01, -1.5612e+01, -8.1018e+00, -3.6050e-02]], device='cuda:1')\n",
      "original u-statistic:-765814.75\n",
      "p value proxy: 1.0\n",
      "result: 0.0\n",
      "power_upto_now: 0.0\n",
      "\n",
      "5th run\n",
      "tensor([[0.8224, 0.7176],\n",
      "        [0.7934, 0.7954],\n",
      "        [0.7127, 0.7856],\n",
      "        [0.6254, 0.7608],\n",
      "        [0.6264, 0.6841]], device='cuda:1')\n",
      "tensor([3, 3, 3, 3, 3], device='cuda:1')\n",
      "tensor([0, 0, 0, 0, 0], device='cuda:1')\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:1')\n",
      "tensor([[ -9.2887,   8.8206,  11.3458,   7.7060],\n",
      "        [ -3.9831,   0.9302,  13.1020, -15.9122],\n",
      "        [ -0.5799, -40.6484,  -8.6854,  34.4224],\n",
      "        [  7.5040,  -7.9679,  12.8615, -18.3324],\n",
      "        [ -5.4470,   1.9926,   0.9241,  22.4640]], device='cuda:1')\n",
      "tensor([[  6.6295,  13.9400,  13.7986,  -1.6341],\n",
      "        [  6.9475,  -4.9179, -10.6755,  19.2679],\n",
      "        [ 36.4869,  45.5488, -17.6568, -20.3157],\n",
      "        [ -1.5937,  -8.7303,  23.5611, -33.5370],\n",
      "        [ 28.0140,  -1.4490, -14.7113,   1.3245]], device='cuda:1')\n",
      "original u-statistic:253907.1875\n",
      "p value proxy: 0.75\n",
      "result: 0.0\n",
      "power_upto_now: 0.0\n",
      "power estimate : 0.0\n",
      "elapsed time: 0.10915327072143555\n",
      "simulation ended at 2022-09-05 20:14:45.997082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester.estimate_power(data_generator = generator, alpha = 1.2, B = 3, n_test = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf809d-5dad-4e48-b5bd-c36d4affb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.1, 1.2, 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905e299-d69c-4743-a271-027c49e8bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_alpha = np.linspace(0.1, 1.2, 41)\n",
    "n_alpha = values_alpha.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e375ea-82fc-438a-9852-dfac093eff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "9658c351-7d02-4391-bd6b-3e556b7d2c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(np.nan, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "baf42eaa-f1f5-4415-a09c-fd41b5b0b054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3000.,  14175.,  25350.,  36525.,  47700.,  58875.,  70050.,\n",
       "        81225.,  92400., 103575., 114750., 125925., 137100., 148275.,\n",
       "       159450., 170625., 181800., 192975., 204150., 215325., 226500.,\n",
       "       237675., 248850., 260025., 271200., 282375., 293550., 304725.,\n",
       "       315900., 327075., 338250., 349425., 360600., 371775., 382950.,\n",
       "       394125., 405300., 416475., 427650., 438825., 450000.])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_n=41\n",
    "values_n = np.linspace(1000, 150000, n_n)*3\n",
    "values_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e1d8a-d3eb-461e-93fd-9d5daf17fa17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
