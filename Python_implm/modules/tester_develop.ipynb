{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e1cb6-af24-4484-b4ee-9d5010d5212d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa2889-f779-4787-a131-b99dd3602f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PrivateMC_Test(TwoSampleTest):\n",
    "    \"\"\"\n",
    "    A generic mean embedding test using a specified kernel, with noise added onto mean, covariance.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_type, test_locs, k, gwidth2, alpha=0.01):\n",
    "        \"\"\"\n",
    "        :param test_locs: J x d numpy array of J locations to test the difference\n",
    "        :param k: a instance of Kernel\n",
    "        \"\"\"\n",
    "        super(PrivateMC_Test, self).__init__(alpha)\n",
    "        self.test_type = test_type\n",
    "        self.gwidth2 = gwidth2\n",
    "        self.test_locs = test_locs\n",
    "        self.k = k\n",
    "\n",
    "    def perform_test(self, tst_data, epsilon=0.5, delta=1e-4, null='private', \n",
    "                     noise='analyse_gauss', gauss_noise='Normal', mean_noise_prop=0.5, seed=23):\n",
    "        stat = self.compute_stat(tst_data, epsilon=epsilon, delta=delta, gauss_noise=gauss_noise,\n",
    "                                 noise=noise, mean_noise_prop=mean_noise_prop, \n",
    "                                 seed=seed)\n",
    "        if self.test_type == 'ME':\n",
    "            J = self.test_locs.shape[0]\n",
    "            kappa = self.k.kappa\n",
    "        elif self.test_type == 'SCF':\n",
    "            J = self.test_locs.shape[0] * 2\n",
    "            kappa = 1.0\n",
    "        alpha = self.alpha\n",
    "        if null in ['private', 'true']:\n",
    "            n = tst_data.len_x()\n",
    "            Sig_pri_pd_inv = inv(self.Sig_private_pd)\n",
    "            U, s, Vh = svd(Sig_pri_pd_inv)\n",
    "            S = np.diag(np.sqrt(s))\n",
    "            Sig_pri_pd_inv_sqrt = np.dot(U, np.dot(S, Vh))\n",
    "            V = self.sigma**2 * np.eye(J)\n",
    "            if null == 'private':\n",
    "                mid = self.Sig_private + n * V\n",
    "            else:\n",
    "                mid = self.Sig + n * V\n",
    "            cov_null = np.matmul(np.matmul(Sig_pri_pd_inv_sqrt, mid), Sig_pri_pd_inv_sqrt)\n",
    "            evals, eV = np.linalg.eig(cov_null)\n",
    "            evals = np.real(evals)\n",
    "            check_evals, check_eV = np.linalg.eig(np.matmul(Sig_pri_pd_inv, mid))\n",
    "            check_2evals, check_2eV = np.linalg.eig(self.Sig_private)\n",
    "            evals[evals < 0] = 0 # numerical instability\n",
    "            pvalue = util.weight_chi_p_value(stat, evals, no_samples=100000)\n",
    "        elif null == 'asymptotic':\n",
    "            pvalue = stats.chi2.sf(stat, J)\n",
    "        else:\n",
    "            raiseValueError('Please specify a correct null formulation.')\n",
    "        results = {'alpha': self.alpha, 'epsilon': epsilon, 'delta': delta,\n",
    "                   'null': null, 'pvalue': pvalue, 'test_stat': stat,\n",
    "                   'h0_rejected': pvalue < alpha, 'kappa': kappa,\n",
    "                   'mean_noise_prop': mean_noise_prop, 'noise': noise}\n",
    "        return results\n",
    "\n",
    "    def compute_stat(self, tst_data, epsilon=0.5, delta=1e-4, noise='analyse_gauss', \n",
    "                     gauss_noise='Normal', mean_noise_prop=0.5, seed=23):\n",
    "        assert 0 < mean_noise_prop < 1.0\n",
    "        lambda_noise_prop = 1.0 - mean_noise_prop\n",
    "        mean_epsilon = epsilon * mean_noise_prop # mean will be epsilon/2.0 private\n",
    "        lambda_epsilon = epsilon * lambda_noise_prop\n",
    "        mean_delta = delta * mean_noise_prop\n",
    "        lambda_delta = delta * lambda_noise_prop\n",
    "        if self.test_locs is None: \n",
    "            raise ValueError('test_locs must be specified.')\n",
    "        X, Y = tst_data.xy()\n",
    "        test_locs = self.test_locs\n",
    "        if self.test_type == 'ME':\n",
    "            k = self.k\n",
    "            kappa = k.kappa\n",
    "            g = k.eval(X, test_locs)\n",
    "            h = k.eval(Y, test_locs)\n",
    "        elif self.test_type == 'SCF':\n",
    "            g = util.construct_z(X, test_locs, self.gwidth2)\n",
    "            h = util.construct_z(Y, test_locs, self.gwidth2)\n",
    "            kappa = 1.0\n",
    "        Z = g-h \n",
    "        n, J = Z.shape\n",
    "        W = np.mean(Z, 0, keepdims=True).T \n",
    "        sensitivity = kappa * sqrt(J) / n\n",
    "        if gauss_noise == 'Normal':\n",
    "            W_private, self.sigma = gauss_mech(W, sensitivity, epsilon=mean_epsilon, \n",
    "                                               delta=mean_delta, return_sigma=True, \n",
    "                                               seed=seed)\n",
    "        elif gauss_noise == 'Improved':\n",
    "            W_private, self.sigma = improve_gauss_mech(W, sensitivity, epsilon=mean_epsilon, \n",
    "                                                       delta=mean_delta, return_sigma=True, \n",
    "                                                       seed=seed)\n",
    "        else:\n",
    "            raise 'gauss_noise: {} not recognised'.format(gauss_noise)\n",
    "        Lambda = np.matmul(Z.T, Z) / n # can change to n-1\n",
    "        if noise == 'analyse_gauss':\n",
    "            sensitivity = (kappa ** 2) * J / (n - 1.0)\n",
    "            Lambda_private = analyse_gauss_mech(Lambda, sensitivity, epsilon=lambda_epsilon, gauss_noise=gauss_noise, \n",
    "                                                delta=lambda_delta, seed=seed+222)\n",
    "        else:\n",
    "            raise 'Noise mech not recognised'\n",
    "        self.Sig_private = util.PSD(Lambda_private - np.matmul(W_private, W_private.T), reg=1e-4)\n",
    "        self.Sig = Lambda - np.matmul(W, W.T)\n",
    "        s_private, self.Sig_private_pd = nc_parameter(n, W_private, self.Sig_private, return_sig=True, reg='auto')\n",
    "        return s_private\n",
    "\n",
    "class PrivateSt_Test(TwoSampleTest):\n",
    "    \"\"\"\n",
    "    A generic mean embedding test using a specified kernel, with noise added onto statistic.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_type, test_locs, k, gwidth2, reg, alpha=0.01):\n",
    "        \"\"\"\n",
    "        :param test_locs: J x d numpy array of J locations to test the difference\n",
    "        :param k: a instance of Kernel\n",
    "        \"\"\"\n",
    "        super(PrivateSt_Test, self).__init__(alpha)\n",
    "        self.test_type = test_type\n",
    "        self.test_locs = test_locs\n",
    "        self.k = k\n",
    "        self.gwidth2 = gwidth2\n",
    "        self.reg = reg\n",
    "\n",
    "    def perform_test(self, tst_data, epsilon=0.5, delta=1e-4, \n",
    "                     gauss_noise='Normal', null='private', seed=23):\n",
    "        n = tst_data.len_x()\n",
    "        if self.test_type == 'ME':\n",
    "            J = self.test_locs.shape[0]\n",
    "            kappa = self.k.kappa\n",
    "        elif self.test_type == 'SCF':\n",
    "            J = self.test_locs.shape[0] * 2\n",
    "            kappa = 1.0\n",
    "        stat = self.compute_stat(tst_data, epsilon=epsilon, delta=delta, \n",
    "                                 gauss_noise=gauss_noise, seed=seed)\n",
    "        if null in ['private', 'true']:\n",
    "            pvalue = util.normal_chi_p_value(stat, J, self.sigma, no_samples=100000)\n",
    "        elif null == 'asymptotic':\n",
    "            pvalue = stats.chi2.sf(stat, J)\n",
    "        else:\n",
    "            raiseValueError('Please specify the correct null formulation.')\n",
    "        alpha = self.alpha\n",
    "        results = {'alpha': alpha, 'epsilon': epsilon, 'delta': delta,\n",
    "                   'null': null, 'pvalue': pvalue, 'test_stat': stat,\n",
    "                   'h0_rejected': pvalue < alpha, 'kappa': kappa}\n",
    "        return results\n",
    "\n",
    "    def compute_stat(self, tst_data, epsilon=0.5, delta=1e-4, gauss_noise='Normal', seed=23):\n",
    "        if self.test_locs is None: \n",
    "            raise ValueError('test_locs must be specified.')\n",
    "        X, Y = tst_data.xy()\n",
    "        test_locs = self.test_locs\n",
    "        if self.test_type == 'ME':\n",
    "            k = self.k\n",
    "            kappa = k.kappa\n",
    "            g = k.eval(X, test_locs)\n",
    "            h = k.eval(Y, test_locs)\n",
    "        elif self.test_type == 'SCF':\n",
    "            g = util.construct_z(X, test_locs, self.gwidth2)\n",
    "            h = util.construct_z(Y, test_locs, self.gwidth2)\n",
    "            kappa = 1.0\n",
    "        Z = g-h \n",
    "        n, J = Z.shape\n",
    "        W = np.mean(Z, 0, keepdims=True).T\n",
    "        Lambda = np.matmul(Z.T, Z) / n\n",
    "        if self.reg == 'auto':\n",
    "            self.reg = 0.00001\n",
    "        Lambda_stable = Lambda + self.reg*np.eye(Lambda.shape[0])\n",
    "        Sig = Lambda_stable - np.matmul(W, W.T)\n",
    "        s = nc_parameter(n, W, Sig) # should be fine!\n",
    "        sensitivity = self.sensitivity_lvl(kappa, J, n, self.reg)\n",
    "        if gauss_noise == 'Normal':\n",
    "            s_private, self.sigma = gauss_mech(s, sensitivity, epsilon=epsilon, \n",
    "                                               delta=delta, return_sigma=True, \n",
    "                                               seed=seed)\n",
    "        elif gauss_noise == 'Improved':\n",
    "            s_private, self.sigma = improve_gauss_mech(s, sensitivity, epsilon=epsilon, \n",
    "                                   delta=delta, return_sigma=True, \n",
    "                                   seed=seed)\n",
    "        else:\n",
    "            raise 'Gauss noise not recognised'\n",
    "        return s_private \n",
    "\n",
    "    @staticmethod\n",
    "    def sensitivity_lvl(kappa, J, n, reg):\n",
    "        B_square = float(kappa**2 * J) / n\n",
    "        print('reg: {}, J: {}, kappa: {}, n: {}'.format(reg, J, kappa, n))\n",
    "        constants = ( 4.0 * kappa**2 * J**(3.0/2.0) )  / n\n",
    "        sensitivity = constants * ((B_square + 1.0) / reg)\n",
    "        print('sensitivity', sensitivity)\n",
    "        return sensitivity\n",
    "\n",
    "class PrivateLocal_PairTest(TwoSampleTest):\n",
    "    \"\"\"\n",
    "    A generic mean embedding (ME) test using a specified kernel\n",
    "    with noise added onto mean and covariance seperately in a local setting\n",
    "    Uses a pair test.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, test_type, test_locs, k, gwidth2, alpha=0.01):\n",
    "        \"\"\"\n",
    "        :param test_locs: J x d numpy array of J locations to test the difference\n",
    "        :param k: a instance of Kernel\n",
    "        \"\"\"\n",
    "        super(PrivateLocal_PairTest, self).__init__(alpha)\n",
    "        self.test_type = test_type\n",
    "        self.test_locs = test_locs\n",
    "        self.k = k\n",
    "        self.gwidth2 = gwidth2\n",
    "\n",
    "    def perform_test(self, tst_data, epsilon=0.5, delta=1e-4, null='private', \n",
    "                     noise='analyse_gauss', gauss_noise='Normal', \n",
    "                     mean_noise_prop=0.5, seed=23):\n",
    "        stat = self.compute_stat(tst_data, epsilon=epsilon, delta=delta, \n",
    "                                 noise=noise, mean_noise_prop=mean_noise_prop, \n",
    "                                 gauss_noise=gauss_noise, seed=seed)\n",
    "        if self.test_type == 'ME':\n",
    "            J = self.test_locs.shape[0]\n",
    "            kappa = self.k.kappa\n",
    "        elif self.test_type == 'SCF':\n",
    "            J = self.test_locs.shape[0] * 2\n",
    "            kappa = 1.0\n",
    "        if null in ['private','true']:\n",
    "            n_x = tst_data.len_x()\n",
    "            n_y = tst_data.len_y()\n",
    "            constant = float(n_x*n_y) / (n_x + n_y)\n",
    "            Sig_pri_pd_inv = inv(self.Sig_private_pd)\n",
    "            U, s, Vh = svd(Sig_pri_pd_inv)\n",
    "            S = np.diag(np.sqrt(s))\n",
    "            Sig_pri_pd_inv_sqrt = np.dot(U, np.dot(S, Vh))\n",
    "            V_x = self.sigma_x**2 * np.eye(J)\n",
    "            V_y = self.sigma_y**2 * np.eye(J)\n",
    "            if null == 'private':\n",
    "                mid = self.Sig_x_private / n_x + self.Sig_y_private / n_y + V_x + V_y \n",
    "            else:\n",
    "                mid = self.Sig_x / n_x + self.Sig_y / n_y + V_x + V_y\n",
    "            cov_null = constant * np.matmul(np.matmul(Sig_pri_pd_inv_sqrt, mid), Sig_pri_pd_inv_sqrt)\n",
    "            evals, eV = np.linalg.eig(cov_null)\n",
    "            evals = np.real(evals)\n",
    "            print(evals)\n",
    "            evals[evals < 0] = 0 # numerical instability\n",
    "            pvalue = util.weight_chi_p_value(stat, evals, no_samples=100000)\n",
    "        elif null == 'asymptotic':\n",
    "            pvalue = stats.chi2.sf(stat, J)\n",
    "        else:\n",
    "            raiseValueError('Please specify correct null formulation')\n",
    "        alpha = self.alpha\n",
    "        results = {'alpha': self.alpha, 'epsilon': epsilon, 'delta': delta,\n",
    "                   'null': null, 'pvalue': pvalue, 'test_stat': stat,\n",
    "                   'h0_rejected': pvalue < alpha, 'kappa': kappa}\n",
    "        return results\n",
    "\n",
    "    def compute_stat(self, tst_data, epsilon=0.5, delta=1e-4, \n",
    "                     noise='analyse_gauss', mean_noise_prop=0.5, \n",
    "                     gauss_noise='Normal', seed=23):\n",
    "        if self.test_locs is None:\n",
    "            raise ValueError('test_locs must be specified.')\n",
    "        X, Y = tst_data.xy()\n",
    "        test_locs = self.test_locs\n",
    "        if self.test_type == 'ME':\n",
    "            k = self.k\n",
    "            kappa = k.kappa\n",
    "            g = k.eval(X, test_locs)\n",
    "            h = k.eval(Y, test_locs)\n",
    "        elif self.test_type == 'SCF':\n",
    "            g = util.construct_z(X, test_locs, self.gwidth2)\n",
    "            h = util.construct_z(Y, test_locs, self.gwidth2)\n",
    "            kappa = 1.0\n",
    "        n_x, W_x_private, Sig_x_private, self.sigma_x, self.Sig_x = self.local_mean_cov(\n",
    "                                                              g, kappa, epsilon=epsilon, gauss_noise=gauss_noise,\n",
    "                                                              delta=delta, mean_noise_prop=mean_noise_prop, \n",
    "                                                              noise=noise, seed=seed)\n",
    "        n_y, W_y_private, Sig_y_private, self.sigma_y, self.Sig_y = self.local_mean_cov(\n",
    "                                                              h, kappa, epsilon=epsilon, gauss_noise=gauss_noise,\n",
    "                                                              delta=delta, mean_noise_prop=mean_noise_prop, \n",
    "                                                              noise=noise, seed=seed+4848)\n",
    "        self.Sig_private = ((n_x - 1.0) * Sig_x_private + (n_y - 1.0) * Sig_y_private) / (n_x + n_y - 2.0)\n",
    "        s, self.Sig_private_pd = nc_parameter_pair(n_x, n_y, W_x_private, W_y_private, \n",
    "                                                   self.Sig_private, return_sig=True, reg='auto')\n",
    "        self.Sig_x_private = Sig_x_private\n",
    "        self.Sig_y_private = Sig_y_private\n",
    "        return s\n",
    "\n",
    "    @staticmethod # TODO: Optimise this\n",
    "    def local_mean_cov(data_k, kappa, epsilon=0.5, delta=1e-4, gauss_noise='Normal',\n",
    "                       mean_noise_prop=0.5, noise='wishart', seed=23):\n",
    "        assert 0 < mean_noise_prop < 1.0\n",
    "        lambda_noise_prop = 1.0 - mean_noise_prop\n",
    "        mean_epsilon = epsilon * mean_noise_prop \n",
    "        lambda_epsilon = epsilon * lambda_noise_prop\n",
    "        mean_delta = delta * mean_noise_prop\n",
    "        lambda_delta = delta * lambda_noise_prop\n",
    "        n, J = data_k.shape\n",
    "        W = np.mean(data_k, 0, keepdims=True).T\n",
    "        Lambda = np.matmul(data_k.T, data_k) / n\n",
    "        sensitivity = kappa * sqrt(J) / n\n",
    "        if gauss_noise == 'Normal':\n",
    "            W_private, sigma = gauss_mech(W, sensitivity, epsilon=mean_epsilon,\n",
    "                                        delta=mean_delta, return_sigma=True, seed=seed)\n",
    "        elif gauss_noise == 'Improved':\n",
    "            W_private, sigma = improve_gauss_mech(W, sensitivity, epsilon=mean_epsilon,\n",
    "                            delta=mean_delta, return_sigma=True, seed=seed)\n",
    "        else:\n",
    "            raise 'Gauss noise not recognised'\n",
    "        if noise == 'analyse_gauss':\n",
    "            sensitivity = (kappa ** 2) * J / (n - 1)\n",
    "            Lambda_private = analyse_gauss_mech(Lambda, sensitivity, epsilon=lambda_epsilon, gauss_noise=gauss_noise, delta=lambda_delta, seed=seed+222)\n",
    "        else:\n",
    "            raise 'Noise mech not recognised'\n",
    "        Sig_private = Lambda_private - np.matmul(W_private, W_private.T)\n",
    "        Sig = Lambda - np.matmul(W, W.T)\n",
    "        Sig_private = util.PSD(Sig_private, reg=1e-4) # Make PD\n",
    "        return n, W_private, Sig_private, sigma, Sig\n",
    "\n",
    "class Local_PairTest(TwoSampleTest):\n",
    "    \"\"\"\n",
    "    A generic mean embedding (ME) test using a specified kernel\n",
    "    in a local setting, uses a pair test.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_locs, k, alpha=0.01):\n",
    "        \"\"\"\n",
    "        :param test_locs: J x d numpy array of J locations to test the difference\n",
    "        :param k: a instance of Kernel\n",
    "        \"\"\"\n",
    "        super(Local_PairTest, self).__init__(alpha)\n",
    "        self.test_locs = test_locs\n",
    "        self.k = k\n",
    "\n",
    "    def perform_test(self, tst_data, epsilon=0.5, delta=1e-4, null='private', \n",
    "                     gauss_noise='Normal', noise='wishart', mean_noise_prop=0.5, seed=23):\n",
    "        stat = self.compute_stat(tst_data, seed=seed)\n",
    "        J, d = self.test_locs.shape\n",
    "        pvalue = stats.chi2.sf(stat, J)\n",
    "        alpha = self.alpha\n",
    "        results = {'alpha': self.alpha, 'epsilon': epsilon, 'delta': delta,\n",
    "                   'null': null, 'pvalue': pvalue, 'test_stat': stat,\n",
    "                   'h0_rejected': pvalue < alpha, 'kappa': self.k.kappa}\n",
    "        return results\n",
    "\n",
    "    def compute_stat(self, tst_data, seed=23):\n",
    "        if self.test_locs is None: \n",
    "            raise ValueError('test_locs must be specified.')\n",
    "        X, Y = tst_data.xy()\n",
    "        test_locs = self.test_locs\n",
    "        k = self.k\n",
    "        J, d = test_locs.shape\n",
    "        g = k.eval(X, test_locs)\n",
    "        h = k.eval(Y, test_locs)\n",
    "        n_x = g.shape[0]\n",
    "        n_y = h.shape[0]\n",
    "        W_x = np.mean(g, 0, keepdims=True).T\n",
    "        W_y = np.mean(h, 0, keepdims=True).T\n",
    "        Lambda_x = np.matmul(g.T, g) / n_x\n",
    "        Lambda_y = np.matmul(h.T, h) / n_y\n",
    "        Sig_x = Lambda_x - np.matmul(W_x, W_x.T)\n",
    "        Sig_y = Lambda_y - np.matmul(W_y, W_y.T)\n",
    "        Sig = ((n_x - 1.0) * Sig_x + (n_y - 1.0) * Sig_y) / (n_x + n_y - 2.0)\n",
    "        s = nc_parameter_pair(n_x, n_y, W_x, W_y, Sig, return_sig=False, reg='auto')\n",
    "        return s\n",
    "\n",
    "class SmoothCFTest(TwoSampleTest):\n",
    "    \"\"\"Class for two-sample test using smooth characteristic functions.\n",
    "    Use Gaussian kernel.\"\"\"\n",
    "    def __init__(self, test_freqs, gaussian_width, alpha=0.01):\n",
    "        \"\"\"\n",
    "        :param test_freqs: J x d numpy array of J frequencies to test the difference\n",
    "        gaussian_width: The width is used to divide the data. The test will be \n",
    "            equivalent if the data is divided beforehand and gaussian_width=1.\n",
    "        \"\"\"\n",
    "        super(SmoothCFTest, self).__init__(alpha)\n",
    "        self.test_freqs = test_freqs\n",
    "        self.gaussian_width = gaussian_width\n",
    "\n",
    "    @property\n",
    "    def gaussian_width(self):\n",
    "        # Gaussian width. Positive number.\n",
    "        return self._gaussian_width\n",
    "    \n",
    "    @gaussian_width.setter\n",
    "    def gaussian_width(self, width):\n",
    "        if util.is_real_num(width) and float(width) > 0:\n",
    "            self._gaussian_width = float(width)\n",
    "        else:\n",
    "            raise ValueError('gaussian_width must be a float > 0. Was %s'%(str(width)))\n",
    "\n",
    "    def compute_stat(self, tst_data):\n",
    "        # test freqs or Gaussian width undefined \n",
    "        if self.test_freqs is None: \n",
    "            raise ValueError('test_freqs must be specified.')\n",
    "\n",
    "        X, Y = tst_data.xy()\n",
    "        test_freqs = self.test_freqs\n",
    "        gamma = self.gaussian_width\n",
    "        s = SmoothCFTest.compute_nc_parameter(X, Y, test_freqs, gamma)\n",
    "        return s\n",
    "\n",
    "    def perform_test(self, tst_data, epsilon=None, delta=None, gauss_noise='Normal', null=None, seed=None):\n",
    "        \"\"\"perform the two-sample test and return values computed in a dictionary:\n",
    "        {alpha: 0.01, pvalue: 0.0002, test_stat: 2.3, h0_rejected: True, ...}\n",
    "        tst_data: an instance of TSTData\n",
    "        \"\"\"\n",
    "        stat = self.compute_stat(tst_data)\n",
    "        J, d = self.test_freqs.shape\n",
    "        # 2J degrees of freedom because of sin and cos\n",
    "        pvalue = stats.chi2.sf(stat, 2*J)\n",
    "        alpha = self.alpha\n",
    "        results = {'alpha': self.alpha, 'pvalue': pvalue, 'test_stat': stat,\n",
    "                'h0_rejected': pvalue < alpha}\n",
    "        return results\n",
    "\n",
    "    #---------------------------------\n",
    "    @staticmethod\n",
    "    def compute_nc_parameter(X, Y, T, gwidth, reg='auto'):\n",
    "        \"\"\"\n",
    "        Compute the non-centrality parameter of the non-central Chi-squared \n",
    "        which is the distribution of the test statistic under the H_1 (and H_0).\n",
    "        The nc parameter is also the test statistic. \n",
    "        \"\"\"\n",
    "        if gwidth is None or gwidth <= 0:\n",
    "            raise ValueError('require gaussian_width > 0. Was %s'%(str(gwidth)))\n",
    "\n",
    "        Z = SmoothCFTest.construct_z(X, Y, T, gwidth)\n",
    "        s = generic_nc_parameter(Z, reg)\n",
    "        return s\n",
    "\n",
    "    @staticmethod\n",
    "    def grid_search_gwidth(tst_data, T, list_gwidth, alpha):\n",
    "        \"\"\"\n",
    "        Linear search for the best Gaussian width in the list that maximizes \n",
    "        the test power, fixing the test locations ot T. \n",
    "        The test power is given by the CDF of a non-central Chi-squared \n",
    "        distribution.\n",
    "        return: (best width index, list of test powers)\n",
    "        \"\"\"\n",
    "        func_nc_param = SmoothCFTest.compute_nc_parameter\n",
    "        J = T.shape[0]\n",
    "        return generic_grid_search_gwidth(tst_data, T, 2*J, list_gwidth, alpha,\n",
    "                func_nc_param)\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def create_randn(tst_data, J, alpha=0.01, seed=19):\n",
    "        \"\"\"Create a SmoothCFTest whose test frequencies are drawn from \n",
    "        the standard Gaussian \"\"\"\n",
    "\n",
    "        rand_state = np.random.get_state()\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        gamma = tst_data.mean_std()*tst_data.dim()**0.5\n",
    "\n",
    "        d = tst_data.dim()\n",
    "        T = np.random.randn(J, d)\n",
    "        np.random.set_state(rand_state)\n",
    "        scf_randn = SmoothCFTest(T, gamma, alpha=alpha)\n",
    "        return scf_randn\n",
    "\n",
    "    @staticmethod \n",
    "    def construct_z(X, Y, test_freqs, gaussian_width):\n",
    "        \"\"\"Construct the features Z to be used for testing with T^2 statistics.\n",
    "        Z is defined in Eq.14 of Chwialkovski et al., 2015 (NIPS). \n",
    "\n",
    "        test_freqs: J x d test frequencies\n",
    "        \n",
    "        Return a n x 2J numpy array. 2J because of sin and cos for each frequency.\n",
    "        \"\"\"\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError('Sample size n must be the same for X and Y.')\n",
    "        X = X/gaussian_width\n",
    "        Y = Y/gaussian_width \n",
    "        n, d = X.shape\n",
    "        J = test_freqs.shape[0]\n",
    "        # inverse Fourier transform (upto scaling) of the unit-width Gaussian kernel \n",
    "        fx = np.exp(-np.sum(X**2, 1)/2)[:, np.newaxis]\n",
    "        fy = np.exp(-np.sum(Y**2, 1)/2)[:, np.newaxis]\n",
    "        # n x J\n",
    "        x_freq = X.dot(test_freqs.T)\n",
    "        y_freq = Y.dot(test_freqs.T)\n",
    "        # zx: n x 2J\n",
    "        zx = np.hstack((np.sin(x_freq)*fx, np.cos(x_freq)*fx))\n",
    "        zy = np.hstack((np.sin(y_freq)*fy, np.cos(y_freq)*fy))\n",
    "        z = zx-zy\n",
    "        assert z.shape == (n, 2*J)\n",
    "        return z\n",
    "\n",
    "    @staticmethod \n",
    "    def construct_z_theano(Xth, Yth, Tth, gwidth_th):\n",
    "        \"\"\"Construct the features Z to be used for testing with T^2 statistics.\n",
    "        Z is defined in Eq.14 of Chwialkovski et al., 2015 (NIPS). \n",
    "        Theano version.\n",
    "        \n",
    "        Return a n x 2J numpy array. 2J because of sin and cos for each frequency.\n",
    "        \"\"\"\n",
    "        Xth = Xth/gwidth_th\n",
    "        Yth = Yth/gwidth_th \n",
    "        # inverse Fourier transform (upto scaling) of the unit-width Gaussian kernel \n",
    "        fx = tensor.exp(-(Xth**2).sum(1)/2).reshape((-1, 1))\n",
    "        fy = tensor.exp(-(Yth**2).sum(1)/2).reshape((-1, 1))\n",
    "        # n x J\n",
    "        x_freq = Xth.dot(Tth.T)\n",
    "        y_freq = Yth.dot(Tth.T)\n",
    "        # zx: n x 2J\n",
    "        zx = tensor.concatenate([tensor.sin(x_freq)*fx, tensor.cos(x_freq)*fx], axis=1)\n",
    "        zy = tensor.concatenate([tensor.sin(y_freq)*fy, tensor.cos(y_freq)*fy], axis=1)\n",
    "        z = zx-zy\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def optimize_freqs_width(tst_data, alpha, n_test_locs=10, max_iter=400,\n",
    "            locs_step_size=0.2, gwidth_step_size=0.01, batch_proportion=1.0,\n",
    "            tol_fun=1e-3, seed=1):\n",
    "        print('n_test_locs', n_test_locs)\n",
    "        \"\"\"Optimize the test frequencies and the Gaussian kernel width by \n",
    "        maximizing the test power. X, Y should not be the same data as used \n",
    "        in the actual test (i.e., should be a held-out set). \n",
    "\n",
    "        - max_iter: #gradient descent iterations\n",
    "        - batch_proportion: (0,1] value to be multipled with nx giving the batch \n",
    "            size in stochastic gradient. 1 = full gradient ascent.\n",
    "        - tol_fun: termination tolerance of the objective value\n",
    "        \n",
    "        Return (test_freqs, gaussian_width, info)\n",
    "        \"\"\"\n",
    "        J = n_test_locs\n",
    "        \"\"\"\n",
    "        Optimize the empirical version of Lambda(T) i.e., the criterion used \n",
    "        to optimize the test locations, for the test based \n",
    "        on difference of mean embeddings with Gaussian kernel. \n",
    "        Also optimize the Gaussian width.\n",
    "\n",
    "        :return a theano function T |-> Lambda(T)\n",
    "        \"\"\"\n",
    "        d = tst_data.dim()\n",
    "        # set the seed\n",
    "        rand_state = np.random.get_state()\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # draw frequencies randomly from the standard Gaussian. \n",
    "        # TODO: Can we do better?\n",
    "        T0 = np.random.randn(J, d)\n",
    "        # reset the seed back to the original\n",
    "        np.random.set_state(rand_state)\n",
    "\n",
    "        # grid search to determine the initial gwidth\n",
    "        mean_sd = tst_data.mean_std()\n",
    "        scales = 2.0**np.linspace(-4, 4, 20)\n",
    "        list_gwidth = np.hstack( (mean_sd*scales*(d**0.5), 2**np.linspace(-7, 7, 20) ))\n",
    "        list_gwidth.sort()\n",
    "        besti, powers = SmoothCFTest.grid_search_gwidth(tst_data, T0,\n",
    "                list_gwidth, alpha)\n",
    "        # initialize with the best width from the grid search\n",
    "        gwidth0 = list_gwidth[besti]\n",
    "        assert util.is_real_num(gwidth0), 'gwidth0 not real. Was %s'%str(gwidth0)\n",
    "        assert gwidth0 > 0, 'gwidth0 not positive. Was %.3g'%gwidth0\n",
    "\n",
    "        func_z = SmoothCFTest.construct_z_theano\n",
    "        # info = optimization info \n",
    "        T, gamma, info = optimize_T_gaussian_width(tst_data, T0, gwidth0, func_z, \n",
    "                max_iter=max_iter, T_step_size=locs_step_size, \n",
    "                gwidth_step_size=gwidth_step_size, batch_proportion=batch_proportion,\n",
    "                tol_fun=tol_fun)\n",
    "        assert util.is_real_num(gamma), 'gamma is not real. Was %s' % str(gamma)\n",
    "\n",
    "        ninfo = {'test_freqs': info['Ts'], 'test_freqs0': info['T0'], \n",
    "                'gwidths': info['gwidths'], 'obj_values': info['obj_values'],\n",
    "                'gwidth0': gwidth0, 'gwidth0_powers': powers}\n",
    "        return (T, gamma, ninfo  )\n",
    "\n",
    "    @staticmethod\n",
    "    def optimize_gwidth(tst_data, T, gwidth0, max_iter=400, \n",
    "            gwidth_step_size=0.1, batch_proportion=1.0, tol_fun=1e-3):\n",
    "        \"\"\"Optimize the Gaussian kernel width by \n",
    "        maximizing the test power, fixing the test frequencies to T. X, Y should\n",
    "        not be the same data as used in the actual test (i.e., should be a\n",
    "        held-out set). \n",
    "\n",
    "        - max_iter: #gradient descent iterations\n",
    "        - batch_proportion: (0,1] value to be multipled with nx giving the batch \n",
    "            size in stochastic gradient. 1 = full gradient ascent.\n",
    "        - tol_fun: termination tolerance of the objective value\n",
    "        \n",
    "        Return (gaussian_width, info)\n",
    "        \"\"\"\n",
    "\n",
    "        func_z = SmoothCFTest.construct_z_theano\n",
    "        # info = optimization info \n",
    "        gamma, info = optimize_gaussian_width(tst_data, T, gwidth0, func_z, \n",
    "                max_iter=max_iter, gwidth_step_size=gwidth_step_size,\n",
    "                batch_proportion=batch_proportion, tol_fun=tol_fun)\n",
    "\n",
    "        ninfo = {'test_freqs': T, 'gwidths': info['gwidths'], 'obj_values':\n",
    "                info['obj_values']}\n",
    "        return ( gamma, ninfo  )\n",
    "\n",
    "\n",
    "class METest(TwoSampleTest):\n",
    "    \"\"\"\n",
    "    A generic mean embedding (ME) test using a specified kernel.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_locs, k, alpha=0.01):\n",
    "        \"\"\"\n",
    "        :param test_locs: J x d numpy array of J locations to test the difference\n",
    "        :param k: a instance of Kernel\n",
    "        \"\"\"\n",
    "        super(METest, self).__init__(alpha)\n",
    "        self.test_locs = test_locs\n",
    "        self.k = k\n",
    "\n",
    "    def perform_test(self, tst_data, epsilon=0.5, delta=1e-4, gauss_noise='Normal', null='asymptotic', seed=23):\n",
    "        stat = self.compute_stat(tst_data)\n",
    "        J, d = self.test_locs.shape\n",
    "        pvalue = stats.chi2.sf(stat, J)\n",
    "        alpha = self.alpha\n",
    "        results = {'alpha': self.alpha, 'pvalue': pvalue, 'test_stat': stat,\n",
    "                'h0_rejected': pvalue < alpha}\n",
    "        return results\n",
    "\n",
    "    def compute_stat(self, tst_data, epsilon=0.5, delta=1e-4):\n",
    "        if self.test_locs is None: \n",
    "            raise ValueError('test_locs must be specified.')\n",
    "\n",
    "        X, Y = tst_data.xy()\n",
    "        test_locs = self.test_locs\n",
    "        k = self.k\n",
    "        g = k.eval(X, test_locs)\n",
    "        h = k.eval(Y, test_locs)\n",
    "        Z = g-h\n",
    "        n = Z.shape[0]\n",
    "        W = np.mean(Z, 0, keepdims=True).T\n",
    "        Lambda = np.matmul(Z.T, Z) / n \n",
    "        Sig = Lambda - np.matmul(W, W.T)\n",
    "        s = nc_parameter(n, W, Sig, reg='auto')\n",
    "        return s\n",
    "\n",
    "#-------------------------------------------------\n",
    "class MeanEmbeddingTest(TwoSampleTest):\n",
    "    \"\"\"Class for two-sample test using squared difference of mean embeddings. \n",
    "    Use Gaussian kernel.\"\"\"\n",
    "\n",
    "    def __init__(self, test_locs, gaussian_width, alpha=0.01):\n",
    "        \"\"\"\n",
    "        :param test_locs: J x d numpy array of J locations to test the difference\n",
    "        gaussian_width: The width is used to divide the data. The test will be \n",
    "            equivalent if the data is divided beforehand and gaussian_width=1.\n",
    "        \"\"\"\n",
    "        # intialise the parent class with siginficance alpaha \n",
    "        super(MeanEmbeddingTest, self).__init__(alpha)\n",
    "\n",
    "        self.test_locs = test_locs\n",
    "        self.gaussian_width = gaussian_width\n",
    "\n",
    "    @property\n",
    "    def gaussian_width(self):\n",
    "        # Gaussian width. Positive number.\n",
    "        return self._gaussian_width\n",
    "    \n",
    "    @gaussian_width.setter\n",
    "    def gaussian_width(self, width):\n",
    "        if util.is_real_num(width) and float(width) > 0:\n",
    "            self._gaussian_width = float(width)\n",
    "        else:\n",
    "            raise ValueError('gaussian_width must be a float > 0. Was %s'%(str(width)))\n",
    "\n",
    "    def perform_test(self, tst_data):\n",
    "        stat = self.compute_stat(tst_data)\n",
    "        #print('stat: %.3g'%stat)\n",
    "        J, d = self.test_locs.shape\n",
    "        pvalue = stats.chi2.sf(stat, J)\n",
    "        alpha = self.alpha\n",
    "        results = {'alpha': self.alpha, 'pvalue': pvalue, 'test_stat': stat,\n",
    "                'h0_rejected': pvalue < alpha}\n",
    "        return results\n",
    "\n",
    "    def compute_stat(self, tst_data):\n",
    "        # test locations or Gaussian width undefined \n",
    "        if self.test_locs is None: \n",
    "            raise ValueError('test_locs must be specified.')\n",
    "\n",
    "        X, Y = tst_data.xy()\n",
    "        test_locs = self.test_locs\n",
    "        gamma = self.gaussian_width\n",
    "        stat = MeanEmbeddingTest.compute_nc_parameter(X, Y, test_locs, gamma)\n",
    "        return stat\n",
    "\n",
    "    def visual_test(self, tst_data):\n",
    "        results = self.perform_test(tst_data)\n",
    "        s = results['test_stat']\n",
    "        pval = results['pvalue']\n",
    "        J = self.test_locs.shape[0]\n",
    "        domain = np.linspace(stats.chi2.ppf(0.001, J), stats.chi2.ppf(0.9999, J), 200)\n",
    "        plt.plot(domain, stats.chi2.pdf(domain, J), label='$\\chi^2$ (df=%d)'%J)\n",
    "        plt.stem([s], [stats.chi2.pdf(J, J)/2], 'or-', label='test stat')\n",
    "        plt.legend(loc='best', frameon=True)\n",
    "        plt.title('%s. p-val: %.3g. stat: %.3g'%(type(self).__name__, pval, s))\n",
    "        plt.show()\n",
    "\n",
    "    #===============================\n",
    "    @staticmethod\n",
    "    def compute_nc_parameter(X, Y, T, gwidth, reg='auto'):\n",
    "        \"\"\"\n",
    "        Compute the non-centrality parameter of the non-central Chi-squared \n",
    "        which is the distribution of the test statistic under the H_1 (and H_0).\n",
    "        The nc parameter is also the test statistic. \n",
    "        \"\"\"\n",
    "        if gwidth is None or gwidth <= 0:\n",
    "            raise ValueError('require gaussian_width > 0. Was %s.'%(str(gwidth)))\n",
    "        n = X.shape[0]\n",
    "        g = MeanEmbeddingTest.gauss_kernel(X, T, gwidth)\n",
    "        h = MeanEmbeddingTest.gauss_kernel(Y, T, gwidth)\n",
    "        Z = g-h\n",
    "        s = generic_nc_parameter(Z, reg)\n",
    "        return s\n",
    "\n",
    "\n",
    "    @staticmethod \n",
    "    def construct_z_theano(Xth, Yth, T, gaussian_width):\n",
    "        \"\"\"Construct the features Z to be used for testing with T^2 statistics.\n",
    "        Z is defined in Eq.12 of Chwialkovski et al., 2015 (NIPS). \n",
    "\n",
    "        T: J x d test locations\n",
    "        \n",
    "        Return a n x J numpy array. \n",
    "        \"\"\"\n",
    "        g = MeanEmbeddingTest.gauss_kernel_theano(Xth, T, gaussian_width)\n",
    "        h = MeanEmbeddingTest.gauss_kernel_theano(Yth, T, gaussian_width)\n",
    "        # Z: nx x J\n",
    "        Z = g-h\n",
    "        return Z\n",
    "\n",
    "    @staticmethod\n",
    "    def gauss_kernel(X, test_locs, gwidth2):\n",
    "        \"\"\"Compute a X.shape[0] x test_locs.shape[0] Gaussian kernel matrix \n",
    "        \"\"\"\n",
    "        n, d = X.shape\n",
    "        D2 = np.sum(X**2, 1)[:, np.newaxis] - 2*X.dot(test_locs.T) + np.sum(test_locs**2, 1)\n",
    "        K = np.exp(-D2/(2.0*gwidth2))\n",
    "        return K\n",
    "\n",
    "    @staticmethod\n",
    "    def gauss_kernel_theano(X, test_locs, gwidth2):\n",
    "        \"\"\"Gaussian kernel for the two sample test. Theano version.\n",
    "        :return kernel matrix X.shape[0] x test_locs.shape[0]\n",
    "        \"\"\"\n",
    "        T = test_locs\n",
    "        n, d = X.shape\n",
    "\n",
    "        D2 = (X**2).sum(1).reshape((-1, 1)) - 2*X.dot(T.T) + tensor.sum(T**2, 1).reshape((1, -1))\n",
    "        K = tensor.exp(-D2/(2.0*gwidth2))\n",
    "        return K\n",
    "\n",
    "    @staticmethod\n",
    "    def create_fit_gauss_heuristic(tst_data, n_test_locs, alpha=0.01, seed=1):\n",
    "        \"\"\"Construct a MeanEmbeddingTest where test_locs are drawn from  Gaussians\n",
    "        fitted to the data x, y.\n",
    "        \"\"\"\n",
    "        #if cov_xy.ndim == 0:\n",
    "        #    # 1d dataset. \n",
    "        #    cov_xy = np.array([[cov_xy]])\n",
    "        X, Y = tst_data.xy()\n",
    "        T = MeanEmbeddingTest.init_locs_2randn(tst_data, n_test_locs, seed)\n",
    "\n",
    "        # Gaussian (asymmetric) kernel width is set to the average standard\n",
    "        # deviations of x, y\n",
    "        #gamma = tst_data.mean_std()*(tst_data.dim()**0.5)\n",
    "        gwidth2 = util.med_sq_distance(tst_data.stack_xy(), 1000)\n",
    "        \n",
    "        met = MeanEmbeddingTest(test_locs=T, gaussian_width=gwidth2, alpha=alpha)\n",
    "        return met\n",
    "\n",
    "    @staticmethod\n",
    "    def optimize_locs_width(tst_data, alpha, n_test_locs=10, max_iter=400, \n",
    "            locs_step_size=0.1, gwidth_step_size=0.01, batch_proportion=1.0, \n",
    "            tol_fun=1e-3, seed=1):\n",
    "        \"\"\"Optimize the test locations and the Gaussian kernel width by \n",
    "        maximizing the test power. X, Y should not be the same data as used \n",
    "        in the actual test (i.e., should be a held-out set). \n",
    "\n",
    "        - max_iter: #gradient descent iterations\n",
    "        - batch_proportion: (0,1] value to be multipled with nx giving the batch \n",
    "            size in stochastic gradient. 1 = full gradient ascent.\n",
    "        - tol_fun: termination tolerance of the objective value\n",
    "        \n",
    "        Return (test_locs, gaussian_width, info)\n",
    "        \"\"\"\n",
    "        J = n_test_locs\n",
    "        \"\"\"\n",
    "        Optimize the empirical version of Lambda(T) i.e., the criterion used \n",
    "        to optimize the test locations, for the test based \n",
    "        on difference of mean embeddings with Gaussian kernel. \n",
    "        Also optimize the Gaussian width.\n",
    "\n",
    "        :return a theano function T |-> Lambda(T)\n",
    "        \"\"\"\n",
    "\n",
    "        med = util.med_sq_distance(tst_data.stack_xy(), 1000)\n",
    "        T0 = MeanEmbeddingTest.init_locs_2randn(tst_data, n_test_locs,\n",
    "                subsample=10000, seed=seed)\n",
    "        #T0 = MeanEmbeddingTest.init_check_subset(tst_data, n_test_locs, med**2,\n",
    "        #      n_cand=30, seed=seed+10)\n",
    "        func_z = MeanEmbeddingTest.construct_z_theano\n",
    "        # Use grid search to initialize the gwidth\n",
    "        list_gwidth2 = np.hstack( ( (med**2) *(2.0**np.linspace(-3, 4, 30) ) ) )\n",
    "        list_gwidth2.sort()\n",
    "        besti, powers = MeanEmbeddingTest.grid_search_gwidth(tst_data, T0,\n",
    "                list_gwidth2, alpha)\n",
    "        gwidth0 = list_gwidth2[besti]\n",
    "        assert util.is_real_num(gwidth0), 'gwidth0 not real. Was %s'%str(gwidth0)\n",
    "        assert gwidth0 > 0, 'gwidth0 not positive. Was %.3g'%gwidth0\n",
    "\n",
    "        # info = optimization info\n",
    "        T, gamma, info = optimize_T_gaussian_width(tst_data, T0, gwidth0, func_z, \n",
    "                max_iter=max_iter, T_step_size=locs_step_size, \n",
    "                gwidth_step_size=gwidth_step_size, batch_proportion=batch_proportion,\n",
    "                tol_fun=tol_fun)\n",
    "        assert util.is_real_num(gamma), 'gamma is not real. Was %s' % str(gamma)\n",
    "\n",
    "        ninfo = {'test_locs': info['Ts'], 'test_locs0': info['T0'], \n",
    "                'gwidths': info['gwidths'], 'obj_values': info['obj_values'],\n",
    "                'gwidth0': gwidth0, 'gwidth0_powers': powers}\n",
    "        return (T, gamma, ninfo  )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def init_check_subset(tst_data, n_test_locs, gwidth2, n_cand=20, subsample=2000,\n",
    "            seed=3):\n",
    "        \"\"\"\n",
    "        Evaluate a set of locations to find the best locations to initialize. \n",
    "        The location candidates are randomly drawn subsets of n_test_locs vectors.\n",
    "        - subsample the data when computing the objective \n",
    "        - n_cand: number of times to draw from the joint and the product \n",
    "            of the marginals.\n",
    "        Return V, W\n",
    "        \"\"\"\n",
    "\n",
    "        X, Y = tst_data.xy()\n",
    "        n = X.shape[0]\n",
    "\n",
    "        # from the joint \n",
    "        objs = np.zeros(n_cand)\n",
    "        seed_seq_joint = util.subsample_ind(7*n_cand, n_cand, seed=seed*5)\n",
    "        for i in range(n_cand):\n",
    "            V = MeanEmbeddingTest.init_locs_subset(tst_data, n_test_locs,\n",
    "                    seed=seed_seq_joint[i])\n",
    "            if subsample < n:\n",
    "                I = util.subsample_ind(n, n_test_locs, seed=seed_seq_joint[i]+1)\n",
    "                XI = X[I, :]\n",
    "                YI = Y[I, :]\n",
    "            else:\n",
    "                XI = X\n",
    "                YI = Y\n",
    "\n",
    "            objs[i] = MeanEmbeddingTest.compute_nc_parameter(XI, YI, V,\n",
    "                    gwidth2, reg='auto')\n",
    "\n",
    "        objs[np.logical_not(np.isfinite(objs))] = -np.infty\n",
    "        # best index \n",
    "        bind = np.argmax(objs)\n",
    "        Vbest = MeanEmbeddingTest.init_locs_subset(tst_data, n_test_locs,\n",
    "                seed=seed_seq_joint[bind])\n",
    "        return Vbest\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def init_locs_subset(tst_data, n_test_locs, seed=2):\n",
    "        \"\"\"\n",
    "        Randomly choose n_test_locs from the union of X and Y in tst_data.\n",
    "        \"\"\"\n",
    "        XY = tst_data.stack_xy()\n",
    "        n2 = XY.shape[0]\n",
    "        I = util.subsample_ind(n2, n_test_locs, seed=seed)\n",
    "        V = XY[I, :]\n",
    "        return V\n",
    "\n",
    "\n",
    "    @staticmethod \n",
    "    def init_locs_randn(tst_data, n_test_locs, seed=1):\n",
    "        \"\"\"Fit a Gaussian to the merged data of the two samples and draw \n",
    "        n_test_locs points from the Gaussian\"\"\"\n",
    "        # set the seed\n",
    "        rand_state = np.random.get_state()\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        X, Y = tst_data.xy()\n",
    "        d = X.shape[1]\n",
    "        # fit a Gaussian in the middle of X, Y and draw sample to initialize T\n",
    "        xy = np.vstack((X, Y))\n",
    "        mean_xy = np.mean(xy, 0)\n",
    "        cov_xy = np.cov(xy.T)\n",
    "        [Dxy, Vxy] = np.linalg.eig(cov_xy + 1e-3*np.eye(d))\n",
    "        Dxy = np.real(Dxy)\n",
    "        Vxy = np.real(Vxy)\n",
    "        Dxy[Dxy<=0] = 1e-3\n",
    "        eig_pow = 0.9 # 1.0 = not shrink\n",
    "        reduced_cov_xy = Vxy.dot(np.diag(Dxy**eig_pow)).dot(Vxy.T) + 1e-3*np.eye(d)\n",
    "\n",
    "        T0 = np.random.multivariate_normal(mean_xy, reduced_cov_xy, n_test_locs)\n",
    "        # reset the seed back to the original\n",
    "        np.random.set_state(rand_state)\n",
    "        return T0\n",
    "\n",
    "    @staticmethod \n",
    "    def init_locs_2randn(tst_data, n_test_locs, subsample=10000, seed=1):\n",
    "        \"\"\"Fit a Gaussian to each dataset and draw half of n_test_locs from \n",
    "        each. This way of initialization can be expensive if the input\n",
    "        dimension is large.\n",
    "        \n",
    "        \"\"\"\n",
    "        if n_test_locs == 1:\n",
    "            return MeanEmbeddingTest.init_locs_randn(tst_data, n_test_locs, seed)\n",
    "\n",
    "        X, Y = tst_data.xy()\n",
    "        n = X.shape[0]\n",
    "        with util.NumpySeedContext(seed=seed):\n",
    "            # Subsample X, Y if needed. Useful if the data are too large.\n",
    "            if n > subsample:\n",
    "                I = util.subsample_ind(n, subsample, seed=seed+2)\n",
    "                X = X[I, :]\n",
    "                Y = Y[I, :]\n",
    "            \n",
    "\n",
    "            d = X.shape[1]\n",
    "            # fit a Gaussian to each of X, Y\n",
    "            mean_x = np.mean(X, 0)\n",
    "            mean_y = np.mean(Y, 0)\n",
    "            cov_x = np.cov(X.T)\n",
    "            [Dx, Vx] = np.linalg.eig(cov_x + 1e-3*np.eye(d))\n",
    "            Dx = np.real(Dx)\n",
    "            Vx = np.real(Vx)\n",
    "            # a hack in case the data are high-dimensional and the covariance matrix \n",
    "            # is low rank.\n",
    "            Dx[Dx<=0] = 1e-3\n",
    "\n",
    "            # shrink the covariance so that the drawn samples will not be so \n",
    "            # far away from the data\n",
    "            eig_pow = 0.9 # 1.0 = not shrink\n",
    "            reduced_cov_x = Vx.dot(np.diag(Dx**eig_pow)).dot(Vx.T) + 1e-3*np.eye(d)\n",
    "            cov_y = np.cov(Y.T)\n",
    "            [Dy, Vy] = np.linalg.eig(cov_y + 1e-3*np.eye(d))\n",
    "            Vy = np.real(Vy)\n",
    "            Dy = np.real(Dy)\n",
    "            Dy[Dy<=0] = 1e-3\n",
    "            reduced_cov_y = Vy.dot(np.diag(Dy**eig_pow).dot(Vy.T)) + 1e-3*np.eye(d)\n",
    "            # integer division\n",
    "            Jx = n_test_locs/2\n",
    "            Jy = n_test_locs - Jx\n",
    "\n",
    "            #from IPython.core.debugger import Tracer\n",
    "            #t = Tracer()\n",
    "            #t()\n",
    "            assert Jx+Jy==n_test_locs, 'total test locations is not n_test_locs'\n",
    "            Tx = np.random.multivariate_normal(mean_x, reduced_cov_x, Jx)\n",
    "            Ty = np.random.multivariate_normal(mean_y, reduced_cov_y, Jy)\n",
    "            T0 = np.vstack((Tx, Ty))\n",
    "\n",
    "        return T0\n",
    "\n",
    "    @staticmethod\n",
    "    def grid_search_gwidth(tst_data, T, list_gwidth, alpha):\n",
    "        \"\"\"\n",
    "        Linear search for the best Gaussian width in the list that maximizes \n",
    "        the test power, fixing the test locations ot T. \n",
    "        return: (best width index, list of test powers)\n",
    "        \"\"\"\n",
    "        func_nc_param = MeanEmbeddingTest.compute_nc_parameter\n",
    "        J = T.shape[0]\n",
    "        return generic_grid_search_gwidth(tst_data, T, J, list_gwidth, alpha,\n",
    "                func_nc_param)\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def optimize_gwidth(tst_data, T, gwidth0, max_iter=400, \n",
    "            gwidth_step_size=0.1, batch_proportion=1.0, tol_fun=1e-3):\n",
    "        \"\"\"Optimize the Gaussian kernel width by \n",
    "        maximizing the test power, fixing the test locations to T. X, Y should\n",
    "        not be the same data as used in the actual test (i.e., should be a\n",
    "        held-out set). \n",
    "\n",
    "        - max_iter: #gradient descent iterations\n",
    "        - batch_proportion: (0,1] value to be multipled with nx giving the batch \n",
    "            size in stochastic gradient. 1 = full gradient ascent.\n",
    "        - tol_fun: termination tolerance of the objective value\n",
    "        \n",
    "        Return (gaussian_width, info)\n",
    "        \"\"\"\n",
    "\n",
    "        func_z = MeanEmbeddingTest.construct_z_theano\n",
    "        # info = optimization info \n",
    "        gamma, info = optimize_gaussian_width(tst_data, T, gwidth0, func_z, \n",
    "                max_iter=max_iter, gwidth_step_size=gwidth_step_size,\n",
    "                batch_proportion=batch_proportion, tol_fun=tol_fun)\n",
    "\n",
    "        ninfo = {'test_locs': T, 'gwidths': info['gwidths'], 'obj_values':\n",
    "                info['obj_values']}\n",
    "        return ( gamma, ninfo  )\n",
    "\n",
    "\n",
    "# ///////////// global functions ///////////////\n",
    "def nc_parameter_pair(n_x, n_y, W_x, W_y, Sig, return_sig=False, reg='auto'):\n",
    "    reg_tolerance = 0.00001\n",
    "    n_features = len(W_x)\n",
    "    W_x = np.squeeze(W_x)\n",
    "    W_y = np.squeeze(W_y)\n",
    "    constant = float(n_x * n_y) / (n_x + n_y)\n",
    "    print('n_x: {}'.format(n_x))\n",
    "    print('n_y: {}'.format(n_y))\n",
    "    if n_features == 1:\n",
    "        reg = 0 if reg=='auto' else reg\n",
    "        s = constant * ( (W_x-W_y)**2 )/(reg+Sig)\n",
    "    else:\n",
    "        W = (W_x - W_y)\n",
    "        if reg=='auto':\n",
    "            # First compute with reg=0. If no problem, do nothing. \n",
    "            # If the covariance is singular, make 0 eigenvalues positive.\n",
    "            try:\n",
    "                evals, eV = np.linalg.eig(Sig)\n",
    "                print(evals)\n",
    "                s = constant*np.linalg.solve(Sig, W).dot(W)\n",
    "            except np.linalg.LinAlgError:\n",
    "                try:\n",
    "                    # singular matrix \n",
    "                    # eigen decompose\n",
    "                    evals, eV = np.linalg.eig(Sig)\n",
    "                    evals = np.real(evals) # due to numerical error potentially \n",
    "                    eV = np.real(eV)\n",
    "                    print('before',evals)\n",
    "                    evals = np.maximum(0, evals)\n",
    "                    # find the non-zero smallest eigenvalue \n",
    "                    ev_small = np.sort(evals[evals > 0])[0]\n",
    "                    evals[evals <= 0] = min(reg_tolerance, ev_small)\n",
    "                    print('after',evals)\n",
    "                    # reconstruct Sig \n",
    "                    Sig = eV.dot(np.diag(evals)).dot(eV.T)\n",
    "                    # try again\n",
    "                    s = constant*np.linalg.solve(Sig, W).dot(W)\n",
    "                except:\n",
    "                    s = -1\n",
    "        else:\n",
    "            # assume reg is a number \n",
    "            # test statistic\n",
    "            try:\n",
    "                s = constant*np.linalg.solve(Sig + reg*np.eye(Sig.shape[0]), W).dot(W)\n",
    "                Sig = Sig + reg*np.eye(Sig.shape[0])\n",
    "                #evals, eV = np.linalg.eig(Sig + reg*np.eye(Sig.shape[0]))\n",
    "                #evals = np.real(evals)\n",
    "                #ev_small = np.sort(evals)[0]\n",
    "            except np.linalg.LinAlgError:\n",
    "                print('LinAlgError. Return -1 as the nc_parameter.')\n",
    "                s = -1\n",
    "        if return_sig:\n",
    "            return s, Sig\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "def nc_parameter(n, W, Sig, return_sig=False, reg='auto'):\n",
    "    reg_tolerance = 0.00001\n",
    "    n_features = len(W)\n",
    "    W = np.squeeze(W)\n",
    "    if n_features == 1: \n",
    "        reg = 0 if reg=='auto' else reg\n",
    "        s = float(n)*(W**2)/(reg+Sig)\n",
    "        ev_small = reg + Sig\n",
    "        Sig = reg + Sig\n",
    "    else:\n",
    "        if reg=='auto':\n",
    "            # First compute with reg=0. If no problem, do nothing. \n",
    "            # If the covariance is singular, make 0 eigenvalues positive.\n",
    "            try:\n",
    "                s = n*np.linalg.solve(Sig, W).dot(W)\n",
    "            except np.linalg.LinAlgError:\n",
    "                try:\n",
    "                    # singular matrix \n",
    "                    # eigen decompose\n",
    "                    evals, eV = np.linalg.eig(Sig)\n",
    "                    evals = np.real(evals) # due to numerical error potentially \n",
    "                    eV = np.real(eV)\n",
    "                    evals = np.maximum(0, evals)\n",
    "                    # find the non-zero smallest eigenvalue \n",
    "                    ev_small = np.sort(evals[evals > 0])[0]\n",
    "                    evals[evals <= 0] = min(reg_tolerance, ev_small) #set to be the one that is smaller.\n",
    "                    # reconstruct Sig \n",
    "                    Sig = eV.dot(np.diag(evals)).dot(eV.T)\n",
    "                    # try again\n",
    "                    s = n*np.linalg.solve(Sig, W).dot(W)\n",
    "                except:\n",
    "                    s = -1\n",
    "        else:\n",
    "            # assume reg is a number \n",
    "            # test statistic\n",
    "            try:\n",
    "                s = n*np.linalg.solve(Sig + reg*np.eye(Sig.shape[0]), W).dot(W)\n",
    "                Sig = Sig + reg*np.eye(Sig.shape[0])\n",
    "                print('fdfdfdd')\n",
    "                #evals, eV = np.linalg.eig(Sig + reg*np.eye(Sig.shape[0]))\n",
    "                #evals = np.real(evals)\n",
    "                #ev_small = np.sort(evals)[0]\n",
    "            except np.linalg.LinAlgError:\n",
    "                print('LinAlgError. Return -1 as the nc_parameter.')\n",
    "                s = -1\n",
    "    if return_sig:\n",
    "        return s, Sig\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def generic_nc_parameter(Z, reg='auto'):\n",
    "    \"\"\"\n",
    "    Compute the non-centrality parameter of the non-central Chi-squared \n",
    "    which is approximately the distribution of the test statistic under the H_1\n",
    "    (and H_0). The empirical nc parameter is also the test statistic. \n",
    "\n",
    "    - reg can be 'auto'. This will automatically determine the lowest value of \n",
    "    the regularization parameter so that the statistic can be computed.\n",
    "    \"\"\"\n",
    "    #from IPython.core.debugger import Tracer \n",
    "    #Tracer()()\n",
    "\n",
    "    n = Z.shape[0]\n",
    "    Sig = np.cov(Z.T)\n",
    "    W = np.mean(Z, 0)\n",
    "    n_features = len(W)\n",
    "    if n_features == 1:\n",
    "        reg = 0 if reg=='auto' else reg\n",
    "        s = float(n)*(W[0]**2)/(reg+Sig)\n",
    "        ev_small = reg + Sig\n",
    "    else:\n",
    "        if reg=='auto':\n",
    "            # First compute with reg=0. If no problem, do nothing. \n",
    "            # If the covariance is singular, make 0 eigenvalues positive.\n",
    "            try:\n",
    "                s = n*np.linalg.solve(Sig, W).dot(W)\n",
    "            except np.linalg.LinAlgError:\n",
    "                try:\n",
    "                    # singular matrix \n",
    "                    # eigen decompose\n",
    "                    evals, eV = np.linalg.eig(Sig)\n",
    "                    evals = np.real(evals)\n",
    "                    eV = np.real(eV)\n",
    "                    evals = np.maximum(0, evals)\n",
    "                    # find the non-zero second smallest eigenvalue\n",
    "                    snd_small = np.sort(evals[evals > 0])[0]\n",
    "                    evals[evals <= 0] = snd_small\n",
    "\n",
    "                    # reconstruct Sig \n",
    "                    Sig = eV.dot(np.diag(evals)).dot(eV.T)\n",
    "                    # try again\n",
    "                    s = n*np.linalg.solve(Sig, W).dot(W)\n",
    "                except:\n",
    "                    s = -1\n",
    "        else:\n",
    "            # assume reg is a number \n",
    "            # test statistic\n",
    "            try:\n",
    "                s = n*np.linalg.solve(Sig + reg*np.eye(Sig.shape[0]), W).dot(W)\n",
    "            except np.linalg.LinAlgError:\n",
    "                print('LinAlgError. Return -1 as the nc_parameter.')\n",
    "                s = -1 \n",
    "    return s\n",
    "\n",
    "def generic_grid_search_gwidth(tst_data, T, df, list_gwidth, alpha, func_nc_param):\n",
    "    \"\"\"\n",
    "    Linear search for the best Gaussian width in the list that maximizes \n",
    "    the test power, fixing the test locations to T. \n",
    "    The test power is given by the CDF of a non-central Chi-squared \n",
    "    distribution.\n",
    "    return: (best width index, list of test powers)\n",
    "    \"\"\"\n",
    "    # number of test locations\n",
    "    X, Y = tst_data.xy()\n",
    "    powers = np.zeros(len(list_gwidth))\n",
    "    lambs = np.zeros(len(list_gwidth))\n",
    "    thresh = stats.chi2.isf(alpha, df=df)\n",
    "    #print('thresh: %.3g'% thresh)\n",
    "    for wi, gwidth in enumerate(list_gwidth):\n",
    "        # non-centrality parameter\n",
    "        try:\n",
    "\n",
    "            #from IPython.core.debugger import Tracer \n",
    "            #Tracer()()\n",
    "            lamb = func_nc_param(X, Y, T, gwidth, reg=0)\n",
    "            if lamb <= 0:\n",
    "                # This can happen when Z, Sig are ill-conditioned. \n",
    "                #print('negative lamb: %.3g'%lamb)\n",
    "                raise np.linalg.LinAlgError\n",
    "            if np.iscomplex(lamb):\n",
    "                # complext value can happen if the covariance is ill-conditioned?\n",
    "                print('Lambda is complex. Truncate the imag part. lamb: %s'%(str(lamb)))\n",
    "                lamb = np.real(lamb)\n",
    "\n",
    "            #print('thresh: %.3g, df: %.3g, nc: %.3g'%(thresh, df, lamb))\n",
    "            power = stats.ncx2.sf(thresh, df=df, nc=lamb)\n",
    "            powers[wi] = power\n",
    "            lambs[wi] = lamb\n",
    "            print('i: %2d, lamb: %5.3g, gwidth: %5.3g, power: %.4f'\n",
    "                   %(wi, lamb, gwidth, power))\n",
    "        except np.linalg.LinAlgError:\n",
    "            # probably matrix inverse failed. \n",
    "            print('LinAlgError. skip width (%d, %.3g)'%(wi, gwidth))\n",
    "            powers[wi] = np.NINF\n",
    "            lambs[wi] = np.NINF\n",
    "    # to prevent the gain of test power from numerical instability, \n",
    "    # consider upto 3 decimal places. Widths that come early in the list \n",
    "    # are preferred if test powers are equal.\n",
    "    besti = np.argmax(np.around(powers, 3))\n",
    "    return besti, powers\n",
    "\n",
    "\n",
    "# Used by SmoothCFTest and MeanEmbeddingTest\n",
    "def optimize_gaussian_width(tst_data, T, gwidth0, func_z, max_iter=400, \n",
    "        gwidth_step_size=0.1, batch_proportion=1.0, \n",
    "        tol_fun=1e-3 ):\n",
    "    \"\"\"Optimize the Gaussian kernel width by gradient ascent \n",
    "    by maximizing the test power.\n",
    "    This does the same thing as optimize_T_gaussian_width() without optimizing \n",
    "    T (T = test locations / test frequencies).\n",
    "\n",
    "    Return (optimized Gaussian width, info)\n",
    "    \"\"\"\n",
    "\n",
    "    X, Y = tst_data.xy()\n",
    "    nx, d = X.shape\n",
    "    # initialize Theano variables\n",
    "    Tth = theano.shared(T, name='T')\n",
    "    Xth = tensor.dmatrix('X')\n",
    "    Yth = tensor.dmatrix('Y')\n",
    "    it = theano.shared(1, name='iter')\n",
    "    # square root of the Gaussian width. Use square root to handle the \n",
    "    # positivity constraint by squaring it later.\n",
    "    gamma_sq_init = gwidth0**0.5\n",
    "    gamma_sq_th = theano.shared(gamma_sq_init, name='gamma')\n",
    "\n",
    "    #sqr(x) = x^2\n",
    "    Z = func_z(Xth, Yth, Tth, tensor.sqr(gamma_sq_th))\n",
    "    W = Z.sum(axis=0)/nx\n",
    "    # covariance \n",
    "    Z0 = Z - W\n",
    "    Sig = Z0.T.dot(Z0)/nx\n",
    "\n",
    "    # gradient computation does not support solve()\n",
    "    #s = slinalg.solve(Sig, W).dot(nx*W)\n",
    "    s = nlinalg.matrix_inverse(Sig).dot(W).dot(W)*nx\n",
    "    gra_gamma_sq = tensor.grad(s, gamma_sq_th)\n",
    "    step_pow = 0.5\n",
    "    max_gam_sq_step = 1.0\n",
    "    func = theano.function(inputs=[Xth, Yth], outputs=s, \n",
    "           updates=[\n",
    "              (it, it+1), \n",
    "              #(gamma_sq_th, gamma_sq_th+gwidth_step_size*gra_gamma_sq\\\n",
    "              #        /it**step_pow/tensor.sum(gra_gamma_sq**2)**0.5 ) \n",
    "              (gamma_sq_th, gamma_sq_th+gwidth_step_size*tensor.sgn(gra_gamma_sq) \\\n",
    "                      *tensor.minimum(tensor.abs_(gra_gamma_sq), max_gam_sq_step) \\\n",
    "                      /it**step_pow) \n",
    "              ] \n",
    "           )\n",
    "    # //////// run gradient ascent //////////////\n",
    "    S = np.zeros(max_iter)\n",
    "    gams = np.zeros(max_iter)\n",
    "    for t in range(max_iter):\n",
    "        # stochastic gradient ascent\n",
    "        ind = np.random.choice(nx, min(int(batch_proportion*nx), nx), replace=False)\n",
    "        # record objective values \n",
    "        S[t] = func(X[ind, :], Y[ind, :])\n",
    "        gams[t] = gamma_sq_th.get_value()**2\n",
    "\n",
    "        # check the change of the objective values \n",
    "        if t >= 2 and abs(S[t]-S[t-1]) <= tol_fun:\n",
    "            break\n",
    "\n",
    "    S = S[:t]\n",
    "    gams = gams[:t]\n",
    "\n",
    "    # optimization info \n",
    "    info = {'T': T, 'gwidths': gams, 'obj_values': S}\n",
    "    return (gams[-1], info  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Used by SmoothCFTest and MeanEmbeddingTest\n",
    "def optimize_T_gaussian_width(tst_data, T0, gwidth0, func_z, max_iter=400, \n",
    "        T_step_size=0.05, gwidth_step_size=0.01, batch_proportion=1.0, \n",
    "        tol_fun=1e-3, reg=1e-5):\n",
    "    \"\"\"Optimize the T (test locations for MeanEmbeddingTest, frequencies for \n",
    "    SmoothCFTest) and the Gaussian kernel width by \n",
    "    maximizing the test power. X, Y should not be the same data as used \n",
    "    in the actual test (i.e., should be a held-out set). \n",
    "    Optimize the empirical version of Lambda(T) i.e., the criterion used \n",
    "    to optimize the test locations.\n",
    "\n",
    "    - T0: Jxd numpy array. initial value of T,  where\n",
    "      J = the number of test locations/frequencies\n",
    "    - gwidth0: initial Gaussian width (width squared for the MeanEmbeddingTest)\n",
    "    - func_z: function that works on Theano variables \n",
    "        to construct features to be used for the T^2 test. \n",
    "        (X, Y, T, gaussian_width) |-> n x J'\n",
    "    - max_iter: #gradient descent iterations\n",
    "    - batch_proportion: (0,1] value to be multipled with nx giving the batch \n",
    "        size in stochastic gradient. 1 = full gradient ascent.\n",
    "    - tol_fun: termination tolerance of the objective value\n",
    "    - reg: a regularization parameter. Must be a non-negative number.\n",
    "    \n",
    "    Return (test_locs, gaussian_width, info)\n",
    "    \"\"\"\n",
    "\n",
    "    #print 'T0: '\n",
    "    #print(T0)\n",
    "    X, Y = tst_data.xy()\n",
    "    nx, d = X.shape\n",
    "    J = T0.shape[0]\n",
    "    # initialize Theano variables\n",
    "    T = theano.shared(T0, name='T')\n",
    "    Xth = tensor.dmatrix('X')\n",
    "    Yth = tensor.dmatrix('Y')\n",
    "    it = theano.shared(1, name='iter')\n",
    "    # square root of the Gaussian width. Use square root to handle the \n",
    "    # positivity constraint by squaring it later.\n",
    "    gamma_sq_init = gwidth0**0.5\n",
    "    gamma_sq_th = theano.shared(gamma_sq_init, name='gamma')\n",
    "    regth = theano.shared(reg, name='reg')\n",
    "    diag_regth = regth*tensor.eye(J)\n",
    "\n",
    "    #sqr(x) = x^2\n",
    "    Z = func_z(Xth, Yth, T, tensor.sqr(gamma_sq_th))\n",
    "    W = Z.sum(axis=0)/nx\n",
    "    # covariance \n",
    "    Z0 = Z - W\n",
    "    Sig = Z0.T.dot(Z0)/nx\n",
    "\n",
    "    # gradient computation does not support solve()\n",
    "    #s = slinalg.solve(Sig, W).dot(nx*W)\n",
    "    s = nlinalg.matrix_inverse(Sig + diag_regth).dot(W).dot(W)*nx\n",
    "    gra_T, gra_gamma_sq = tensor.grad(s, [T, gamma_sq_th])\n",
    "    step_pow = 0.5\n",
    "    max_gam_sq_step = 1.0\n",
    "    func = theano.function(inputs=[Xth, Yth], outputs=s, \n",
    "           updates=[\n",
    "              (T, T+T_step_size*gra_T/it**step_pow/tensor.sum(gra_T**2)**0.5 ), \n",
    "              (it, it+1), \n",
    "              #(gamma_sq_th, gamma_sq_th+gwidth_step_size*gra_gamma_sq\\\n",
    "              #        /it**step_pow/tensor.sum(gra_gamma_sq**2)**0.5 ) \n",
    "              (gamma_sq_th, gamma_sq_th+gwidth_step_size*tensor.sgn(gra_gamma_sq) \\\n",
    "                      *tensor.minimum(tensor.abs_(gra_gamma_sq), max_gam_sq_step) \\\n",
    "                      /it**step_pow) \n",
    "              ] \n",
    "           )\n",
    "           #updates=[(T, T+T_step_size*gra_T), (it, it+1), \n",
    "           #    (gamma_sq_th, gamma_sq_th+gwidth_step_size*gra_gamma_sq) ] )\n",
    "                           #updates=[(T, T+0.1*gra_T), (it, it+1) ] )\n",
    "\n",
    "    # //////// run gradient ascent //////////////\n",
    "    S = np.zeros(max_iter)\n",
    "    J = T0.shape[0]\n",
    "    Ts = np.zeros((max_iter, J, d))\n",
    "    gams = np.zeros(max_iter)\n",
    "    for t in range(max_iter):\n",
    "        # stochastic gradient ascent\n",
    "        ind = np.random.choice(nx, min(int(batch_proportion*nx), nx), replace=False)\n",
    "        # record objective values \n",
    "        try:\n",
    "            S[t] = func(X[ind, :], Y[ind, :])\n",
    "        except: \n",
    "            print('Exception occurred during gradient descent. Stop optimization.')\n",
    "            print('Return the value from previous iter. ')\n",
    "            import traceback as tb \n",
    "            tb.print_exc()\n",
    "            t = t -1\n",
    "            break\n",
    "\n",
    "        Ts[t] = T.get_value()\n",
    "        gams[t] = gamma_sq_th.get_value()**2\n",
    "\n",
    "        # check the change of the objective values \n",
    "        if t >= 2 and abs(S[t]-S[t-1]) <= tol_fun:\n",
    "            break\n",
    "\n",
    "    S = S[:t+1]\n",
    "    Ts = Ts[:t+1]\n",
    "    gams = gams[:t+1]\n",
    "\n",
    "    # optimization info \n",
    "    info = {'Ts': Ts, 'T0':T0, 'gwidths': gams, 'obj_values': S, 'gwidth0':\n",
    "            gwidth0}\n",
    "\n",
    "    if t >= 0:\n",
    "        opt_T = Ts[-1]\n",
    "        # for some reason, optimization can give a non-numerical result\n",
    "        opt_gwidth = gams[-1] if util.is_real_num(gams[-1]) else gwidth0\n",
    "\n",
    "        if np.linalg.norm(opt_T) <= 1e-5:\n",
    "            opt_T = T0\n",
    "            opt_gwidth = gwidth0\n",
    "    else:\n",
    "        # Probably an error occurred in the first iter.\n",
    "        opt_T = T0\n",
    "        opt_gwidth = gwidth0\n",
    "    return (opt_T, opt_gwidth, info  )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33707fd9-34d6-49b1-83b9-2494e7b02bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2], [2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f261de-9b7f-43d2-894d-8797a01f88d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74bce373-879d-4f42-bf39-b2c61d8e5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module containing many types of two sample test algorithms\n",
    "The structure and part of this code was adapted from Wittawat Jitkrittum\n",
    "Linear-time interpretable nonparametric two-sample test\"\"\"\n",
    "__author__ = \"leon\"\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from private_me.data import TSTData\n",
    "#import private_me.util as util\n",
    "#import private_me.kernel as kernel\n",
    "#from private_me.private_mechanism import gauss_mech, improve_gauss_mech, analyse_gauss_mech\n",
    "#from scipy.linalg import block_diag, sqrtm, inv, svd\n",
    "\n",
    "\n",
    "from scipy.linalg import block_diag, sqrtm, inv, svd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class tester(object):\n",
    "    \"\"\"Abstract class for two sample tests.\"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, gamma, cuda_device, seed):\n",
    "        \"\"\"\n",
    "        gamma: significance level of the test\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.cuda_device = cuda_device\n",
    "        self.seed = seed\n",
    "    \n",
    "    @abstractmethod   \n",
    "    def estimate_power(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def permu_test(self):\n",
    "        \"\"\"perform the two-sample test and return values computed in a dictionary:\n",
    "        {alpha: 0.01, pvalue: 0.0002, test_stat: 2.3, h0_rejected: True, ...}\n",
    "        tst_data: an instance of TSTData\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_stat(self):\n",
    "        \"\"\"Compute the test statistic\"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def privatize(slef):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def LapU(self, data, d, alpha, c):\n",
    "        ''' Only for continuous data.\n",
    "        for each dimension, transform the data in [0,1] into the interval index\n",
    "        first interval = [0, x], the others = (y z]\n",
    "        \n",
    "        input arguments\n",
    "            data: torch tensor object on GPU of multivariate data\n",
    "            d: number of categories of multivariate data\n",
    "            alpha: privacy level\n",
    "            c: noise scale paramter\n",
    "        output\n",
    "            LDPView: \\alpha-LDP view of the input multivariate data\n",
    "        '''\n",
    " \n",
    "        sigma = c / alpha\n",
    "        oneHot = self.transform_onehot(dataMultivariate, d)\n",
    "        laplaceSize = oneHot.size()\n",
    "        laplaceNoise = generate_unit_laplace(laplaceSize)\n",
    "        LDPView = torch.sqrt(d) * oneHot + sigma * laplaceNoise\n",
    "        return(LDPView)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def h_bin(self, data, kappa): \n",
    "        ''' Only for continuous data\n",
    "        input arguments\n",
    "            data: torch tensor of continuous data\n",
    "            kappa: number of bin in each dimension\n",
    "        output\n",
    "            torch tensor of multivariate data\n",
    "        '''\n",
    "               \n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "     \n",
    "        # 1. for each dimension, turn the continuous data into interval\n",
    "        # each row now indicates a hypercube in [0,1]^d\n",
    "        # the more the data is closer to 1, the larger the interval index.\n",
    "        dataBinIndex = self.transform_bin_index(data = data, nIntervals = kappa)\n",
    "        \n",
    "        # 2. for each datapoint(row),\n",
    "        #    turn the hypercube data into a multivariate data of (1, 2, ..., kappa^d)\n",
    "        #    each row now becomes an integer.\n",
    "        dataMultivariate = self.TransformMultivariate(\n",
    "            dataInterval = dataBinIndex,\n",
    "            nBin = kappa,\n",
    "        )\n",
    "        \n",
    "        return(dataMultivariate)\n",
    "    \n",
    "    def transform_bin_index(self, data, nIntervals):\n",
    "        ''' Only for continuous data.\n",
    "        for each dimension, transform the data in [0,1] into the interval index\n",
    "        first interval = [0, x], the others = (y z]\n",
    "        \n",
    "        input arguments\n",
    "            data: torch tensor object on GPU\n",
    "            nIntervals: integer\n",
    "        output\n",
    "            dataIndices: torch tensor, dimension same as the input\n",
    "        '''\n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "        breaks = torch.linspace(start = 0, end = 1, steps = nIntervals + 1).to(self.cuda_device) #floatTensor\n",
    "        dataIndices = torch.bucketize(data, breaks, right = False) # ( ] form.\n",
    "        dataIndices = dataIndices.add(\n",
    "            dataIndices.eq(0)\n",
    "        ) #move 0 values from the bin number 0 to the bin number 1        \n",
    "        return(dataIndices)    \n",
    "\n",
    "    def TransformMultivariate(self, dataBinIndex, nBin):\n",
    "        \"\"\"Only for continuous and multivariate data .\"\"\"\n",
    "        d = self.get_dimension(dataBinIndex)\n",
    "        \n",
    "        if d == 1:\n",
    "            return(dataInterval.sub(1))\n",
    "        else:\n",
    "            exponent = torch.linspace(start = (d-1), end = 0, steps = d, dtype = torch.long)\n",
    "            vector = torch.tensor(nBin).pow(exponent)\n",
    "            return( torch.matmul( dataInterval.sub(1).to(torch.float), vector.to(torch.float).to(self.cuda_device) ).to(torch.long) )\n",
    "    \n",
    "    \n",
    "    def generate_unit_laplace(self, size):\n",
    "        '''\n",
    "        input: torch.size object\n",
    "        output: torch tensor of data from unit laplace distribution\n",
    "        '''\n",
    "     \n",
    "        n = data.size(dim = 0)\n",
    "        d = data.size(dim = 1)\n",
    "        unit_laplace_generator = torch.distributions.laplace.Laplace(\n",
    "            torch.tensor(0.0).to(self.cuda_device),\n",
    "            torch.tensor(2**(-1/2)).to(self.cuda_device)\n",
    "        )\n",
    "        return unit_laplace_generator.sample(sample_shape = size)\n",
    "        \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_onehot(dataMultivariate, d):\n",
    "        return(\n",
    "            torch.nn.functional.one_hot(\n",
    "                dataMultivariate,\n",
    "                num_classes = d)\n",
    "        )\n",
    " \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_dimension(data):\n",
    "        if data.dim() == 1:\n",
    "            return(1)\n",
    "        elif data.dim() == 2:\n",
    "            return( data.size(dim = 1) )\n",
    "        else:\n",
    "            return # we only use up to 2-dimensional tensor, i.e. matrix\n",
    "\n",
    "    @staticmethod        \n",
    "    def range_check(self, data):\n",
    "        if (torch.sum(data.gt(1))).gt(0):\n",
    "            print(\"check data range\")\n",
    "            return False\n",
    "        elif (torch.sum(data.lt(0))).gt(0):\n",
    "            print(\"check data range\")\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf9ce2-72e8-4229-91f0-b8d18937d26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "786ef310-5117-4818-884f-e00a0aee7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(object):\n",
    "    \"\"\"Abstract class for two sample tests.\"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, cuda_device, seed):\n",
    "        self.cuda_device = cuda_device\n",
    "        self.seed = seed\n",
    "        self.cdf_calculator = torch.distributions.normal.Normal(loc = 0.0, scale = 1.0)\n",
    "    \n",
    "    @abstractmethod   \n",
    "    def generate_y(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @abstractmethod   \n",
    "    def generate_z(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "    def calculate_cdf(data):\n",
    "        return self.cdf_calculator(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67063220-41fc-4f53-bf7f-c2e987069a9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'kappa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mtwoSampleContiTester\u001b[39;00m(tester):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gamma, cuda_device, seed, kappa):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(twoSampleContiTester, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(gamma, cuda_device, seed)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'kappa'"
     ]
    }
   ],
   "source": [
    "class twoSampleContiTester(tester):\n",
    "    def __init__(self, gamma, cuda_device, seed, kappa):\n",
    "        super(twoSampleContiTester, self).__init__(gamma, cuda_device, seed)\n",
    "        self.kappa = kappa\n",
    "    \n",
    "    def estimate_power(self, data_generator, alpha, B, n_test):\n",
    "        torch.manual_seed(0)\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        print(f\"\"\"\n",
    "        simulation started at = {datetime.datetime.now()} \\n\n",
    "        n = {n},\\n\n",
    "        kappa = {kappa}, alpha = {alpha},\\n\n",
    "        gamma = {gamma}, nTests = {nTests},\\n\n",
    "        B = {B}, d = {d}\n",
    "        \"\"\")\n",
    "        test_results = torch.empty(n_test)\n",
    "        \n",
    "        for rep in range(nTests):\n",
    "            print(f\"\\n{rep+1}th run\")\n",
    "            tst_data_y = data_generator.generate_y()\n",
    "            tst_data_z = data_generator.generate_z()\n",
    "            test_results[rep] = self.permu_test(tst_data_y, tst_data_z, alpha = 0.8, B = 300)\n",
    "            print(f\"result: {test_results[rep]}\")\n",
    "            print(f\"power_upto_now: { torch.sum(test_results[:(rep+1)])/(rep+1) }\")\n",
    "  \n",
    "        print( f\"power estimate : { torch.sum(test_results)/nTests }\" )\n",
    "        print( f\"elapsed time: { time.time() - start_time }\" )\n",
    "        print( f\"simulation ended at {datetime.datetime.now()}\" )\n",
    "        return(torch.sum(\n",
    "        \n",
    "    def permu_test(self, tst_data_y, tst_data_z, alpha, B): \n",
    "        n_1 = tst_data_y.size(dim = 0)\n",
    "        tst_data_y_priv, tst_data_z_priv = self.privatize(tst_data_y, tst_data_z, alpha)\n",
    "        dataCombined = torch.cat([tst_data_y_priv, tst_data_z_priv], dim = 0)\n",
    " \n",
    "        #original statistic\n",
    "        ustatOriginal = self.compute_stat(dataCombined[:n_1,:], dataCombined[n_1:,:])\n",
    "        print(f\"original u-statistic:{ustatOriginal}\")\n",
    "        \n",
    "        #permutation procedure\n",
    "        permStats = torch.empty(B).to(self.cuda_device)\n",
    "        \n",
    "        for i in range(B):\n",
    "            perm_stat_now = self.compute_stat(\n",
    "                dataCombined[torch.randperm(dataPrivatized.size(dim=0))][:n_1,:],\n",
    "                dataCombined[torch.randperm(dataPrivatized.size(dim=0))][n_1:,:]\n",
    "            ).to(self.cuda_device)\n",
    "            permStats[i] = perm_stat_now\n",
    "\n",
    "               \n",
    "        p_value_proxy = (1 +\n",
    "                         torch.sum(\n",
    "                             torch.gt(input = permStats, other = ustatOriginal)\n",
    "                         )\n",
    "                        ) / (B + 1)\n",
    "      \n",
    "        print(f\"p value proxy: {p_value_proxy}\")\n",
    "        return(p_value_proxy < self.gamma)#test result: TRUE = 1 = reject the null, FALSE = 0 = retain the null.    \n",
    " \n",
    "    def compute_stat(self, tst_data_y_priv, tst_data_z_priv):\n",
    "        n_1 = tst_data_y_priv.size(dim = 0) \n",
    "        n_2 = tst_data_z_priv.size(dim = 0)\n",
    "    \n",
    "        y_row_sum = torch.sum(tst_data_y_priv, axis = 0)\n",
    "        z_row_sum = torch.sum(tst_data_z_priv, axis = 0)\n",
    "        phi_psi = torch.einsum('ji,jk->ik', tst_data_y_priv, tst_data_z_priv)\n",
    "\n",
    "\n",
    "        one_Phi_one = torch.inner(y_row_sum, y_row_sum)\n",
    "        one_Psi_one = torch.inner(z_row_sum, z_row_sum)\n",
    "\n",
    "        tr_Phi = torch.sum(torch.square(tst_data_y_priv))\n",
    "        tr_Psi = torch.sum(torch.square(tst_data_z_priv))\n",
    "\n",
    "        one_Phi_tilde_one = one_Phi_one - tr_Phi\n",
    "        one_Psi_tilde_one = one_Psi_one - tr_Psi\n",
    "\n",
    "        onePhioneonePsione = one_Phi_tilde_one * one_Psi_tilde_one\n",
    "\n",
    "        # y only part. log calculation in case of large n1\n",
    "        sign_y = torch.sign(one_Phi_tilde_one)\n",
    "        abs_u_y = torch.exp(torch.log(torch.abs(one_Phi_tilde_one)) - torch.log(n_1) - torch.log(n_1 - 1) )\n",
    "        u_y = sign_y * abs_u_y\n",
    "\n",
    "\n",
    "        # z only part. log calculation in case of large n2\n",
    "        sign_z = torch.sign(one_Psi_tilde_one)\n",
    "\n",
    "        abs_u_z = torch.exp(torch.log(torch.abs(one_Psi_tilde_one)) - torch.log(n_2) - torch.log(n_2 - 1) )\n",
    "        u_z = sign_z * abs_u_z\n",
    "\n",
    "        # cross part\n",
    "        cross = torch.inner(y_row_sum, z_row_sum)\n",
    "        sign_cross = torch.sign(cross)\n",
    "        abs_cross = torch.exp(torch.log(torch.abs(cross)) +torch.log(torch.tensor(2))- torch.log(n_1) - torch.log(n_2) )\n",
    "        u_cross = sign_cross * abs_cross\n",
    "\n",
    "        return(u_y + u_z - u_cross)\n",
    "    \n",
    "        \n",
    "    def privatize(self, tst_data_y, tst_data_z, alpha):\n",
    "        d = self.kappa ** tst_data_y.size(dim = 1)\n",
    "        c = torch.sqrt(8 * d)\n",
    "        tst_data_y_multi = self.h_bin(tst_data_y, self.kappa)\n",
    "        tst_data_z_multi = self.h_bin(tst_data_z, self.kappa) \n",
    "        tst_data_y_priv = LapU(self, tst_data_y_multi, d, alpha, c)\n",
    "        tst_data_z_priv = LapU(self, tst_data_z_multi, d, alpha, c)\n",
    "        return(tst_data_y_priv, tst_data_z_priv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f244c7c-72b0-4b43-b884-c1e3fb7022f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def permu_test(self, tst_data, alpha = 0.8, B = 300, seed=23):\n",
    "        #0. data range check\n",
    "        \n",
    "        if not self.range_check(data_Y):\n",
    "            return\n",
    "        if not self.range_check(data_Z):\n",
    "            return\n",
    "        \n",
    "        #1. bin\n",
    "        n = data_Y.size(dim = 0)\n",
    "        data_Y_binned, data_Z_binned = self.bin_separately(data_Y, data_Z, kappa)\n",
    "\n",
    "        #2. privatize\n",
    "        data_Y_priv, data_Z_priv, noise_var_Y, noise_var_Z = self.privatize_indep(\n",
    "            data_Y = data_Y_binned,\n",
    "            data_Z = data_Z_binned,\n",
    "            alpha = alpha,\n",
    "            discrete_noise = discrete_noise\n",
    "        )\n",
    "        \n",
    "        #4 compute original u-stat\n",
    "        ustatOriginal = self.u_stat_indep_matrix_efficient(data_Y_priv, data_Z_priv)\n",
    "\n",
    "        #print(f\"original u-statistic:{ustatOriginal}\")\n",
    "        \n",
    "        #permutation procedure\n",
    "        permStats = torch.empty(B).to(self.cuda_device)\n",
    "        \n",
    "        for i in range(B):\n",
    "            perm_stat_now = self.u_stat_indep_matrix_efficient(\n",
    "                data_Y_priv,\n",
    "                data_Z_priv[\n",
    "                    torch.randperm(data_Z_priv.size(dim=0))],\n",
    "                ).to(self.cuda_device)\n",
    "\n",
    "            permStats[i] = perm_stat_now\n",
    "            #print(f\"perm_stat_now = {perm_stat_now}\")\n",
    "         \n",
    "        \n",
    "        p_value_proxy = (1 +\n",
    "                         torch.sum(\n",
    "                             torch.gt(input = permStats, other = ustatOriginal)\n",
    "                         )\n",
    "                        ) / (B + 1)\n",
    "        \n",
    "        \n",
    "        #print(f\"p value proxy: {p_value_proxy}\")\n",
    "        \n",
    "        return(p_value_proxy < gamma, noise_var_Y, noise_var_Z)#test result: TRUE = 1 = reject the null, FALSE = 0 = retain the null.\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca0ac1-6680-4b3a-835f-2e21eddc5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_sample_generator_mean_departure(data_generator):\n",
    "    def __init__(self, cuda_device, seed, n1, n2, d):\n",
    "        super(two_sample_generator_mean_departure, self).__init__(cuda_device, seed)\n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "        self.d = d\n",
    "\n",
    "        copula_mean_y = -1/2 * torch.ones(d).to(self.cuda_device)\n",
    "        copula_mean_z =  1/2 * torch.ones(d).to(self.cuda_device)\n",
    "\n",
    "        sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(self.cuda_device)\n",
    "\n",
    "\n",
    "        print(\"copula_mean_y\")\n",
    "        print(copula_mean_1)\n",
    "\n",
    "        print(\"copula_mean_z\")\n",
    "        print(copula_mean_2)\n",
    "\n",
    "        print(\"sigma\")\n",
    "        print(sigma)\n",
    "\n",
    "        self.generator_y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean_y, \n",
    "            covariance_matrix = sigma)\n",
    "        self.generator_z = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean_z,\n",
    "            covariance_matrix = sigma)\n",
    "        \n",
    "        def generate_y(self):\n",
    "            return(\n",
    "                self.calculate_cdf(\n",
    "                    self.generator_y.sample( (self.n1,) )\n",
    "                )\n",
    "            )       \n",
    "        def generate_z(self):\n",
    "            return(\n",
    "                self.calculate_cdf(\n",
    "                    self.generator_z.sample( (self.n2,) )\n",
    "                )\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "355808e9-ddd8-42ac-8279-ad938d4762d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "code run on device:: cuda:1\n",
      "copula_mean_y\n",
      "tensor([-0.5000, -0.5000, -0.5000, -0.5000], device='cuda:1')\n",
      "copula_mean_z\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000], device='cuda:1')\n",
      "sigma\n",
      "tensor([[1.0000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 1.0000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 1.0000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 1.0000]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available() \n",
    "print(f\"cuda available: {USE_CUDA}\")\n",
    "\n",
    "device = torch.device('cuda:1' if USE_CUDA else 'cpu') \n",
    "print(f\"code run on device:: {device}\")\n",
    "tester = twoSampleContiTester(gamma = 0.05, cuda_device = device, seed = 0, kappa = 3)\n",
    "\n",
    "generator = two_sample_generator_mean_departure(cuda_device = device, seed = 0, n1 = 10000, n2 = 10000, d = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8193bb-1a77-47b9-ba86-96f0a84b3043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
