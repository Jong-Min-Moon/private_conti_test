{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "469fbe7b-78a6-466e-ab3a-79ab17cf9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from private_me.data import TSTData\n",
    "#import private_me.util as util\n",
    "#import private_me.kernel as kernel\n",
    "#from private_me.private_mechanism import gauss_mech, improve_gauss_mech, analyse_gauss_mech\n",
    "#from scipy.linalg import block_diag, sqrtm, inv, svd\n",
    "\n",
    "\n",
    "from scipy.linalg import block_diag, sqrtm, inv, svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7e00b4-9f7d-47ef-a1cd-5596e448787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tester(object):\n",
    "    \"\"\"Abstract class for two sample tests.\"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, gamma, cuda_device, seed):\n",
    "        \"\"\"\n",
    "        gamma: significance level of the test\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.cuda_device = cuda_device\n",
    "        self.seed = seed\n",
    "    \n",
    "\n",
    "    \n",
    "    @abstractmethod\n",
    "    def permu_test(self):\n",
    "        \"\"\"perform the two-sample test and return values computed in a dictionary:\n",
    "        {alpha: 0.01, pvalue: 0.0002, test_stat: 2.3, h0_rejected: True, ...}\n",
    "        tst_data: an instance of TSTData\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_stat(self):\n",
    "        \"\"\"Compute the test statistic\"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @abstractmethod\n",
    "    def privatize(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def LapU(self, oneHot, alpha, c, theta):\n",
    "        ''' Only for continuous data.\n",
    "        for each dimension, transform the data in [0,1] into the interval index\n",
    "        first interval = [0, x], the others = (y z]\n",
    "        \n",
    "        input arguments\n",
    "            data: torch tensor object on GPU of multivariate data\n",
    "            d: number of categories of multivariate data\n",
    "            alpha: privacy level\n",
    "            c: noise scale paramter\n",
    "        output\n",
    "            LDPView: \\alpha-LDP view of the input multivariate data\n",
    "        '''\n",
    " \n",
    "        sigma = c * 2**(1/2) * theta / alpha\n",
    "        laplaceSize = oneHot.size()\n",
    "        laplaceNoise = self.generate_unit_laplace(laplaceSize)\n",
    "        LDPView = torch.tensor(theta) * oneHot + sigma * laplaceNoise\n",
    "        return(LDPView)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def h_bin(self, data, kappa): \n",
    "        ''' Only for continuous data\n",
    "        input arguments\n",
    "            data: torch tensor of continuous data\n",
    "            kappa: number of bin in each dimension\n",
    "        output\n",
    "            torch tensor of multivariate data\n",
    "        '''\n",
    "               \n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "     \n",
    "        # 1. for each dimension, turn the continuous data into interval\n",
    "        # each row now indicates a hypercube in [0,1]^d\n",
    "        # the more the data is closer to 1, the larger the interval index.\n",
    "        dataBinIndex = self.transform_bin_index(data = data, nIntervals = kappa)\n",
    "        \n",
    "        # 2. for each datapoint(row),\n",
    "        #    turn the hypercube data into a multivariate data of (1, 2, ..., kappa^d)\n",
    "        #    each row now becomes an integer.\n",
    "        dataMultivariate = self.TransformMultivariate(dataBinIndex, kappa)\n",
    "        print(dataMultivariate)\n",
    "        return(dataMultivariate)\n",
    "    \n",
    "    def transform_bin_index(self, data, nIntervals):\n",
    "        ''' Only for continuous data.\n",
    "        for each dimension, transform the data in [0,1] into the interval index\n",
    "        first interval = [0, x], the others = (y z]\n",
    "        \n",
    "        input arguments\n",
    "            data: torch tensor object on GPU\n",
    "            nIntervals: integer\n",
    "        output\n",
    "            dataIndices: torch tensor, dimension same as the input\n",
    "        '''\n",
    "        # create designated number of intervals\n",
    "        d = self.get_dimension(data)\n",
    "        breaks = torch.linspace(start = 0, end = 1, steps = nIntervals + 1).to(self.cuda_device) #floatTensor\n",
    "        dataIndices = torch.bucketize(data, breaks, right = False) # ( ] form.\n",
    "        dataIndices = dataIndices.add(\n",
    "            dataIndices.eq(0)\n",
    "        ) #move 0 values from the bin number 0 to the bin number 1        \n",
    "        return(dataIndices)    \n",
    "\n",
    "    def TransformMultivariate(self, dataBinIndex, nBin):\n",
    "        \"\"\"Only for continuous and multivariate data .\"\"\"\n",
    "        d = self.get_dimension(dataBinIndex)\n",
    "        \n",
    "        if d == 1:\n",
    "            return(dataInterval.sub(1))\n",
    "        else:\n",
    "            exponent = torch.linspace(start = (d-1), end = 0, steps = d, dtype = torch.long)\n",
    "            vector = torch.tensor(nBin).pow(exponent)\n",
    "            return( torch.matmul( dataBinIndex.sub(1).to(torch.float), vector.to(torch.float).to(self.cuda_device) ).to(torch.long) )\n",
    "    \n",
    "    \n",
    "    def generate_unit_laplace(self, size):\n",
    "        '''\n",
    "        input: torch.size object\n",
    "        output: torch tensor of data from unit laplace distribution\n",
    "        '''\n",
    "     \n",
    "        unit_laplace_generator = torch.distributions.laplace.Laplace(\n",
    "            torch.tensor(0.0).to(self.cuda_device),\n",
    "            torch.tensor(2**(-1/2)).to(self.cuda_device)\n",
    "        )\n",
    "        return unit_laplace_generator.sample(sample_shape = size)\n",
    "        \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_onehot(dataMultivariate, d):\n",
    "        return(\n",
    "            torch.nn.functional.one_hot(\n",
    "                dataMultivariate,\n",
    "                num_classes = d)\n",
    "        )\n",
    " \n",
    "\n",
    "    @staticmethod\n",
    "    def get_dimension(data):\n",
    "        if data.dim() == 1:\n",
    "            return(1)\n",
    "        elif data.dim() == 2:\n",
    "            return( data.size(dim = 1) )\n",
    "        else:\n",
    "            return # we only use up to 2-dimensional tensor, i.e. matrix\n",
    "\n",
    "    @staticmethod        \n",
    "    def range_check(self, data):\n",
    "        if (torch.sum(data.gt(1))).gt(0):\n",
    "            print(\"check data range\")\n",
    "            return False\n",
    "        elif (torch.sum(data.lt(0))).gt(0):\n",
    "            print(\"check data range\")\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4de135c-51fe-4d7c-857f-34f6a8a8384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(object):\n",
    "    \"\"\"Abstract class for two sample tests.\"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, cuda_device, seed):\n",
    "        self.cuda_device = cuda_device\n",
    "        self.seed = seed\n",
    "        self.cdf_calculator = torch.distributions.normal.Normal(loc = 0.0, scale = 1.0)\n",
    "    \n",
    "   # @abstractmethod   \n",
    "    #def generate_y(self):\n",
    "        #raise NotImplementedError(\"implement generate_y\")\n",
    "        \n",
    "    @abstractmethod   \n",
    "    def generate_z(self):\n",
    "        raise NotImplementedError(\"implement generate_z\")\n",
    "        \n",
    "    def calculate_cdf(self, data):\n",
    "        return self.cdf_calculator.cdf(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9f6a5d0-cd07-4e39-b741-95e966159a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoSampleContiTester(tester):\n",
    "    def __init__(self, gamma, cuda_device, seed, kappa):\n",
    "        super(twoSampleContiTester, self).__init__(gamma, cuda_device, seed)\n",
    "        self.kappa = kappa\n",
    "    \n",
    "    def estimate_power(self, data_generator, alpha, B, n_test):\n",
    "        torch.manual_seed(0)\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        start_time = time.time()\n",
    "        print(f\"\"\"\n",
    "        simulation started at = {datetime.datetime.now()} \\n\n",
    "        n1 = {data_generator.n1}, n2 = {data_generator.n2}, \\n\n",
    "        kappa = {self.kappa}, alpha = {alpha},\\n\n",
    "        gamma = {self.gamma}, nTests = {n_test},\\n\n",
    "        B = {B}, d = {data_generator.d}\n",
    "        \"\"\")\n",
    "        test_results = torch.empty(n_test)\n",
    "        \n",
    "        for rep in range(n_test):\n",
    "            print(f\"\\n{rep+1}th run\")\n",
    "            tst_data_y = data_generator.generate_y()\n",
    "            tst_data_z = data_generator.generate_z()\n",
    "            test_results[rep] = self.permu_test(tst_data_y, tst_data_z, alpha, B)\n",
    "            print(f\"result: {test_results[rep]}\")\n",
    "            print(f\"power_upto_now: { torch.sum(test_results[:(rep+1)])/(rep+1) }\")\n",
    "  \n",
    "        print( f\"power estimate : { torch.sum(test_results)/n_test }\" )\n",
    "        print( f\"elapsed time: { time.time() - start_time }\" )\n",
    "        print( f\"simulation ended at {datetime.datetime.now()}\" )\n",
    "        return(torch.sum(test_results)/n_test)\n",
    "        \n",
    "    def permu_test(self, tst_data_y, tst_data_z, alpha, B): \n",
    "        n_1 = tst_data_y.size(dim = 0)\n",
    "        tst_data_priv = self.privatize(tst_data_y, tst_data_z, alpha)\n",
    "        n = tst_data_priv.size(dim = 0)\n",
    "        \n",
    "        #original statistic\n",
    "        ustatOriginal = self.compute_stat(tst_data_priv[:n_1,:], tst_data_priv[n_1:,:])\n",
    "        print(f\"original u-statistic:{ustatOriginal}\")\n",
    "        \n",
    "        #permutation procedure\n",
    "        permStats = torch.empty(B).to(self.cuda_device)\n",
    "        \n",
    "        for i in range(B):\n",
    "            permutation = torch.randperm(n)\n",
    "            perm_stat_now = self.compute_stat(\n",
    "                tst_data_priv[permutation][:n_1,:],\n",
    "                tst_data_priv[permutation][n_1:,:]\n",
    "            ).to(self.cuda_device)\n",
    "            permStats[i] = perm_stat_now\n",
    "\n",
    "               \n",
    "        p_value_proxy = (1 +\n",
    "                         torch.sum(\n",
    "                             torch.gt(input = permStats, other = ustatOriginal)\n",
    "                         )\n",
    "                        ) / (B + 1)\n",
    "      \n",
    "        print(f\"p value proxy: {p_value_proxy}\")\n",
    "        return(p_value_proxy < self.gamma)#test result: TRUE = 1 = reject the null, FALSE = 0 = retain the null.    \n",
    " \n",
    "    def compute_stat(self, tst_data_y_priv, tst_data_z_priv):\n",
    "        n_1 = torch.tensor(tst_data_y_priv.size(dim = 0))\n",
    "        n_2 = torch.tensor(tst_data_z_priv.size(dim = 0))\n",
    "    \n",
    "        y_row_sum = torch.sum(tst_data_y_priv, axis = 0)\n",
    "        z_row_sum = torch.sum(tst_data_z_priv, axis = 0)\n",
    "        phi_psi = torch.einsum('ji,jk->ik', tst_data_y_priv, tst_data_z_priv)\n",
    "\n",
    "\n",
    "        one_Phi_one = torch.inner(y_row_sum, y_row_sum)\n",
    "        one_Psi_one = torch.inner(z_row_sum, z_row_sum)\n",
    "\n",
    "        tr_Phi = torch.sum(torch.square(tst_data_y_priv))\n",
    "        tr_Psi = torch.sum(torch.square(tst_data_z_priv))\n",
    "\n",
    "        one_Phi_tilde_one = one_Phi_one - tr_Phi\n",
    "        one_Psi_tilde_one = one_Psi_one - tr_Psi\n",
    "\n",
    "        onePhioneonePsione = one_Phi_tilde_one * one_Psi_tilde_one\n",
    "\n",
    "        # y only part. log calculation in case of large n1\n",
    "        sign_y = torch.sign(one_Phi_tilde_one)\n",
    "        abs_u_y = torch.exp(torch.log(torch.abs(one_Phi_tilde_one)) - torch.log(n_1) - torch.log(n_1 - 1) )\n",
    "        u_y = sign_y * abs_u_y\n",
    "\n",
    "\n",
    "        # z only part. log calculation in case of large n2\n",
    "        sign_z = torch.sign(one_Psi_tilde_one)\n",
    "\n",
    "        abs_u_z = torch.exp(torch.log(torch.abs(one_Psi_tilde_one)) - torch.log(n_2) - torch.log(n_2 - 1) )\n",
    "        u_z = sign_z * abs_u_z\n",
    "\n",
    "        # cross part\n",
    "        cross = torch.inner(y_row_sum, z_row_sum)\n",
    "        sign_cross = torch.sign(cross)\n",
    "        abs_cross = torch.exp(torch.log(torch.abs(cross)) +torch.log(torch.tensor(2))- torch.log(n_1) - torch.log(n_2) )\n",
    "        u_cross = sign_cross * abs_cross\n",
    "\n",
    "        return(u_y + u_z - u_cross)\n",
    "    \n",
    "        \n",
    "    def privatize(self, tst_data_y, tst_data_z, alpha):\n",
    "        d = self.kappa ** tst_data_y.size(dim = 1)\n",
    "        theta = d**(1/2)\n",
    "        tst_data_y_multi = self.h_bin(tst_data_y, self.kappa)\n",
    "        tst_data_y_oneHot = self.transform_onehot(tst_data_y_multi, d)\n",
    "        tst_data_z_multi = self.h_bin(tst_data_z, self.kappa) \n",
    "        tst_data_z_oneHot = self.transform_onehot(tst_data_z_multi, d)\n",
    "        dataCombined = torch.cat([tst_data_y_oneHot, tst_data_z_oneHot], dim = 0)\n",
    "        tst_data_priv = self.LapU(dataCombined, alpha, 2, theta)\n",
    "        return(tst_data_priv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5a39186-97f9-4c19-949c-0e19b253e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndepContiTester(tester):\n",
    "    def __init__(self, gamma, cuda_device, seed, kappa):\n",
    "        super(IndepContiTester, self).__init__(gamma, cuda_device, seed)\n",
    "        self.kappa = kappa\n",
    "    \n",
    "    def estimate_power(self, data_generator, alpha, B, n_test):\n",
    "        torch.manual_seed(0)\n",
    "        random.seed(0)\n",
    "        np.random.seed(0)\n",
    "        start_time = time.time()\n",
    "        print(f\"\"\"\n",
    "        simulation started at = {datetime.datetime.now()} \\n\n",
    "        n = {data_generator.n}, \\n\n",
    "        kappa = {self.kappa}, alpha = {alpha},\\n\n",
    "        gamma = {self.gamma}, nTests = {n_test},\\n\n",
    "        B = {B}, d = {data_generator.d}\n",
    "        \"\"\")\n",
    "        \n",
    "        test_results = torch.empty(n_test)\n",
    "        \n",
    "        for rep in range(n_test):\n",
    "            print(f\"\\n{rep+1}th run\")\n",
    "            tst_data_y = data_generator.generate_y()\n",
    "            tst_data_z = data_generator.generate_z()\n",
    "            test_results[rep] = self.permu_test(tst_data_y, tst_data_z, alpha, B)\n",
    "            print(f\"result: {test_results[rep]}\")\n",
    "            print(f\"power_upto_now: { torch.sum(test_results[:(rep+1)])/(rep+1) }\")\n",
    "  \n",
    "        print( f\"power estimate : { torch.sum(test_results)/n_test }\" )\n",
    "        print( f\"elapsed time: { time.time() - start_time }\" )\n",
    "        print( f\"simulation ended at {datetime.datetime.now()}\" )\n",
    "        return(torch.sum(test_results)/n_test)\n",
    "\n",
    "    #done\n",
    "    def permu_test(self, tst_data_y, tst_data_z, alpha, B): \n",
    "        n = tst_data_z.size(dim = 0)\n",
    "        tst_data_priv_y, tst_data_priv_z = self.privatize(tst_data_y, tst_data_z, alpha)\n",
    "        print(tst_data_priv_y)\n",
    "        print(tst_data_priv_z)\n",
    "        \n",
    "        #original statistic\n",
    "        ustatOriginal = self.compute_stat(tst_data_priv_y, tst_data_priv_z)\n",
    "        print(f\"original u-statistic:{ustatOriginal}\")\n",
    "        \n",
    "        #permutation procedure\n",
    "        permStats = torch.empty(B).to(self.cuda_device)\n",
    "        \n",
    "        for i in range(B):\n",
    "            permutation = torch.randperm(n)\n",
    "            perm_stat_now = self.compute_stat(\n",
    "                tst_data_priv_y,\n",
    "                tst_data_priv_z[permutation]\n",
    "            ).to(self.cuda_device)\n",
    "            permStats[i] = perm_stat_now\n",
    "\n",
    "               \n",
    "        p_value_proxy = (1 +\n",
    "                         torch.sum(\n",
    "                             torch.gt(input = permStats, other = ustatOriginal)\n",
    "                         )\n",
    "                        ) / (B + 1)\n",
    "      \n",
    "        print(f\"p value proxy: {p_value_proxy}\")\n",
    "        return(p_value_proxy < self.gamma)#test result: TRUE = 1 = reject the null, FALSE = 0 = retain the null.    \n",
    "\n",
    "    #done\n",
    "    def compute_stat(self, tst_data_y_priv, tst_data_z_priv):\n",
    "        #scalars\n",
    "        n = tst_data_y_priv.size(dim = 0)\n",
    "        \n",
    "        log_n_four = (\n",
    "        torch.log(torch.tensor(n))\n",
    "        +  \n",
    "        torch.log(torch.tensor(n-1))\n",
    "        +\n",
    "        torch.log(torch.tensor(n-2))\n",
    "        +\n",
    "        torch.log(torch.tensor(n-3))\n",
    "        )\n",
    "\n",
    "        #preliminary calculations\n",
    "        y_row_sum = torch.sum(tst_data_y_priv, axis = 0)\n",
    "        z_row_sum = torch.sum(tst_data_z_priv, axis = 0)\n",
    "        phi_psi = torch.einsum('ji,jk->ik', tst_data_y_priv, tst_data_z_priv)\n",
    "        diag_Phi = torch.sum(torch.square(tst_data_y_priv), axis = 1)\n",
    "        diag_Psi = torch.sum(torch.square(tst_data_z_priv), axis = 1)\n",
    "        rowsum_Phi = torch.einsum('i,ji -> j', y_row_sum, tst_data_y_priv)\n",
    "        rowsum_Psi = torch.einsum('ij, j -> i', tst_data_z_priv, z_row_sum)\n",
    "\n",
    "        #1. one term\n",
    "        one_Phi_one = torch.inner(y_row_sum, y_row_sum)\n",
    "        one_Psi_one = torch.inner(z_row_sum, z_row_sum)\n",
    "\n",
    "        tr_Phi = torch.sum(torch.square(tst_data_y_priv))\n",
    "        tr_Psi = torch.sum(torch.square(tst_data_z_priv))\n",
    "\n",
    "        one_Phi_tilde_one = one_Phi_one - tr_Phi\n",
    "        one_Psi_tilde_one = one_Psi_one - tr_Psi\n",
    "\n",
    "        onePhioneonePsione = one_Phi_tilde_one * one_Psi_tilde_one\n",
    "\n",
    "\n",
    "        #2. one one term\n",
    "        onePhiPsiOne = torch.matmul(\n",
    "            torch.matmul(y_row_sum, phi_psi),\n",
    "            z_row_sum)  + torch.inner(diag_Phi, diag_Psi)-torch.inner(rowsum_Phi, diag_Psi)-torch.inner(diag_Phi, rowsum_Psi)\n",
    "\n",
    "\n",
    "        #3. trace term\n",
    "        trPhiPsi = torch.sum( torch.square(phi_psi) ) - torch.inner(\n",
    "            torch.sum( torch.square(tst_data_y_priv), axis = 1),\n",
    "            torch.sum( torch.square(tst_data_z_priv), axis = 1)\n",
    "        )\n",
    "        \n",
    "        sums = (4 * onePhioneonePsione - ( 8 * (n-1) ) * onePhiPsiOne + ( 4 * (n-1) * (n-2) ) * trPhiPsi )\n",
    "        \n",
    "        Un_sign = torch.sign(sums)\n",
    "        abs_Un = torch.exp(torch.log(torch.abs(sums)) - log_n_four)\n",
    "        Un = Un_sign * abs_Un\n",
    "\n",
    "        return(Un)\n",
    "    \n",
    "      #done  \n",
    "    def privatize(self, tst_data_y, tst_data_z, alpha):\n",
    "        d1 = self.kappa ** tst_data_y.size(dim = 1)\n",
    "        d2 = self.kappa ** tst_data_z.size(dim = 1)\n",
    "        theta = (d1*d2)**(1/2)\n",
    "        print(tst_data_y)\n",
    "        tst_data_y_multi = self.h_bin(tst_data_y, self.kappa)\n",
    "        tst_data_z_multi = self.h_bin(tst_data_z, self.kappa) \n",
    "        \n",
    "        \n",
    "        tst_data_y_oneHot = self.transform_onehot(tst_data_y_multi, d1)\n",
    "        print(tst_data_y_oneHot)\n",
    "        tst_data_z_oneHot = self.transform_onehot(tst_data_z_multi, d2)\n",
    "\n",
    "        tst_data_priv_y = self.LapU(tst_data_y_oneHot, alpha, 4, theta)\n",
    "        tst_data_priv_z = self.LapU(tst_data_z_oneHot, alpha, 4, theta)\n",
    "        return(tst_data_priv_y, tst_data_priv_z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a0f86c-7e08-4a4a-9e39-1b7f27346dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_sample_generator_mean_departure(data_generator):\n",
    "    def __init__(self, cuda_device, seed, n1, n2, d):\n",
    "        super(two_sample_generator_mean_departure, self).__init__(cuda_device, seed)\n",
    "        self.n1 = n1\n",
    "        self.n2 = n2\n",
    "        self.d = d\n",
    "\n",
    "        copula_mean_y = -1/2 * torch.ones(d).to(self.cuda_device)\n",
    "        copula_mean_z =  1/2 * torch.ones(d).to(self.cuda_device)\n",
    "\n",
    "        sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(self.cuda_device)\n",
    "\n",
    "\n",
    "        print(\"copula_mean_y\")\n",
    "        print(copula_mean_y)\n",
    "\n",
    "        print(\"copula_mean_z\")\n",
    "        print(copula_mean_z)\n",
    "\n",
    "        print(\"sigma\")\n",
    "        print(sigma)\n",
    "\n",
    "        self.generator_y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean_y, \n",
    "            covariance_matrix = sigma)\n",
    "        self.generator_z = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean_z,\n",
    "            covariance_matrix = sigma)\n",
    "        \n",
    "    def generate_y(self):\n",
    "            normalSample = self.generator_y.sample( (self.n1,) )\n",
    "            return( self.calculate_cdf(normalSample) )  \n",
    "        \n",
    "    def generate_z(self):\n",
    "            return(\n",
    "                self.calculate_cdf(\n",
    "                    self.generator_z.sample( (self.n2,) )\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44de59b8-5c3f-4d51-8e82-70d1dbc47724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class indep_generator_trivial(data_generator):\n",
    "    def __init__(self, cuda_device, seed, n, d):\n",
    "        super(indep_generator_trivial, self).__init__(cuda_device, seed)\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.normalsample = 0\n",
    "        copula_mean = -1/2 * torch.ones(d).to(self.cuda_device)\n",
    "\n",
    "        sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(self.cuda_device)\n",
    "\n",
    "\n",
    "        print(\"copula_mean\")\n",
    "        print(copula_mean)\n",
    "\n",
    "\n",
    "        print(\"sigma\")\n",
    "        print(sigma)\n",
    "\n",
    "        self.generator_y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean, \n",
    "            covariance_matrix = sigma)\n",
    "\n",
    "        \n",
    "    def generate_y(self):\n",
    "            #self.normalSample = self.generator_y.sample( (self.n,) )\n",
    "            self.normalSample = torch.tensor(\n",
    "            [[0.9244, 0.5756],\n",
    "        [0.8182, 0.8254],\n",
    "        [0.5614, 0.7913],\n",
    "        [0.3196, 0.7090],\n",
    "        [0.3224, 0.4793]]\n",
    "            ).to(self.cuda_device)\n",
    "            return( self.calculate_cdf(self.normalSample) )  \n",
    "        \n",
    "    def generate_z(self):\n",
    "            return(\n",
    "                self.calculate_cdf(\n",
    "                    -self.normalSample\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe57dba-70b4-4d83-b523-a4b71abd09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class indep_generator_nontrivial(data_generator):\n",
    "    def __init__(self, cuda_device, seed, n, d):\n",
    "        super(indep_generator_trivial, self).__init__(cuda_device, seed)\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.normalsample = 0\n",
    "        copula_mean = -1/2 * torch.ones(d).to(self.cuda_device)\n",
    "\n",
    "        sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(self.cuda_device)\n",
    "\n",
    "\n",
    "        print(\"copula_mean\")\n",
    "        print(copula_mean)\n",
    "\n",
    "\n",
    "        print(\"sigma\")\n",
    "        print(sigma)\n",
    "\n",
    "        self.generator_y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean, \n",
    "            covariance_matrix = sigma)\n",
    "\n",
    "        \n",
    "    def generate_y(self):\n",
    "        self.normalSample = self.generator_y.sample( (self.n,) )\n",
    "        return( self.calculate_cdf(self.normalSample) )  \n",
    "        \n",
    "    def generate_z(self):\n",
    "        return(\n",
    "            self.calculate_cdf(\n",
    "                -self.normalSample\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4288a19c-be43-49cc-ac06-b331265c5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class indep_generator_nontrivial(data_generator):\n",
    "    def __init__(self, cuda_device, seed, n, d):\n",
    "        super(indep_generator_trivial, self).__init__(cuda_device, seed)\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.normalsample = 0\n",
    "        copula_mean = -1/2 * torch.ones(d).to(self.cuda_device)\n",
    "\n",
    "        sigma = (0.5 * torch.ones(d,d) + 0.5 * torch.eye(d)).to(self.cuda_device)\n",
    "\n",
    "\n",
    "        print(\"copula_mean\")\n",
    "        print(copula_mean)\n",
    "\n",
    "\n",
    "        print(\"sigma\")\n",
    "        print(sigma)\n",
    "\n",
    "        self.generator_y = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            loc = copula_mean, \n",
    "            covariance_matrix = sigma)\n",
    "\n",
    "        \n",
    "    def generate_y(self):\n",
    "        self.normalSample = self.generator_y.sample( (self.n,) )\n",
    "        return( self.calculate_cdf(self.normalSample) )  \n",
    "        \n",
    "    def generate_z(self):\n",
    "        return(\n",
    "            self.calculate_cdf(\n",
    "                torch.sin(self.normalSample) + 0.05\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55db41-9fa2-4ebe-9c5f-cf82cea111f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available() \n",
    "print(f\"cuda available: {USE_CUDA}\")\n",
    "\n",
    "device = torch.device('cuda:1' if USE_CUDA else 'cpu') \n",
    "print(f\"code run on device:: {device}\")\n",
    "tester = IndepContiTester(gamma = 0.05, cuda_device = device, seed = 0, kappa = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64ce2021-2c39-45c5-9916-62761c1d8336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copula_mean\n",
      "tensor([-0.5000, -0.5000], device='cuda:1')\n",
      "sigma\n",
      "tensor([[1.0000, 0.5000],\n",
      "        [0.5000, 1.0000]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "generator = indep_generator_trivial(cuda_device = device, seed = 0, n = 5, d = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9147f35-e0f2-465d-abe6-bf312295ffde",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'tester' has no attribute 'estimate_power'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_power\u001b[49m(data_generator \u001b[38;5;241m=\u001b[39m generator, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.2\u001b[39m, B \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'tester' has no attribute 'estimate_power'"
     ]
    }
   ],
   "source": [
    "tester.estimate_power(data_generator = generator, alpha = 1.2, B = 3, n_test = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf809d-5dad-4e48-b5bd-c36d4affb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.1, 1.2, 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905e299-d69c-4743-a271-027c49e8bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_alpha = np.linspace(0.1, 1.2, 41)\n",
    "n_alpha = values_alpha.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e375ea-82fc-438a-9852-dfac093eff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "9658c351-7d02-4391-bd6b-3e556b7d2c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(np.nan, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "baf42eaa-f1f5-4415-a09c-fd41b5b0b054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3000.,  14175.,  25350.,  36525.,  47700.,  58875.,  70050.,\n",
       "        81225.,  92400., 103575., 114750., 125925., 137100., 148275.,\n",
       "       159450., 170625., 181800., 192975., 204150., 215325., 226500.,\n",
       "       237675., 248850., 260025., 271200., 282375., 293550., 304725.,\n",
       "       315900., 327075., 338250., 349425., 360600., 371775., 382950.,\n",
       "       394125., 405300., 416475., 427650., 438825., 450000.])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_n=41\n",
    "values_n = np.linspace(1000, 150000, n_n)*3\n",
    "values_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e1d8a-d3eb-461e-93fd-9d5daf17fa17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
